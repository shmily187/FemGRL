import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
from torch.utils.checkpoint import checkpoint  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜
from datetime import timedelta  # ç”¨äºDDPè¶…æ—¶è®¾ç½®
import matplotlib.pyplot as plt
# æ··åˆç²¾åº¦è®­ç»ƒå·²ç¦ç”¨ï¼Œä½¿ç”¨çº¯float32è®­ç»ƒä»¥ä¿è¯ç¨³å®šæ€§
from torch_geometric.loader import DataListLoader # ã€å…³é”®ã€‘ä½¿ç”¨ ListLoader
import torch.distributed as dist                    # DDP åˆ†å¸ƒå¼è®­ç»ƒ
import torch.multiprocessing as mp                  # å¤šè¿›ç¨‹
from torch.nn.parallel import DistributedDataParallel as DDP  # DDP
from torch_geometric.nn import DataParallel         # å…¼å®¹æ€§ä¿ç•™
from torch_geometric.data import Batch, Data
import numpy as np
import os
import time
import copy
import json
from sklearn.model_selection import train_test_split

# ==========================================
# 1. å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
# ==========================================
from build_graph import build_graph_data, scan_all_data, root_data_path, load_file_data, is_main_process
from model import BuildGCNList
import os
# DDPç¯å¢ƒä¼˜åŒ–ï¼šé¿å…å¤šè¿›ç¨‹æ•°æ®åŠ è½½å¯¼è‡´çš„èµ„æºç«äº‰
# åœ¨DDPä¸­ï¼Œæ¯ä¸ªGPUè¿›ç¨‹éƒ½ä¼šåˆ›å»ºnum_workersä¸ªå­è¿›ç¨‹
# å¦‚æœæœ‰Nä¸ªGPUï¼Œnum_workers=4ï¼Œåˆ™æ€»å…±æœ‰4*Nä¸ªæ•°æ®åŠ è½½å­è¿›ç¨‹
# è¿™ä¼šå¯¼è‡´æ–‡ä»¶æè¿°ç¬¦è€—å°½å’Œå…±äº«å†…å­˜å¯¹è±¡è¿‡å¤š
# å»ºè®®ï¼šDDPç¯å¢ƒä¸­è®¾ç½®num_workers=0ï¼Œè®©ä¸»è¿›ç¨‹å¤„ç†æ•°æ®åŠ è½½
NUM_WORKERS = 0  # DDPç¯å¢ƒï¼šç¦ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä»¥é¿å…èµ„æºç«äº‰

# ==========================================
# 2. è®­ç»ƒé…ç½®
# ==========================================
# å­¦ä¹ ç‡è°ƒåº¦é…ç½®
# ==========================================
# ReduceLROnPlateau é…ç½®
REDUCE_LR_PATIENCE = 30  # å¤šå°‘ä¸ªepochæ— æ”¹å–„å°±é™ä½å­¦ä¹ ç‡
REDUCE_LR_FACTOR = 0.5   # å­¦ä¹ ç‡é™ä½å› å­
REDUCE_LR_MIN_LR = 1e-6  # æœ€å°å­¦ä¹ ç‡
REDUCE_LR_MODE = 'min'   # 'min'è¡¨ç¤ºç›‘æ§æŒ‡æ ‡è¶Šå°è¶Šå¥½
# REDUCE_LR_VERBOSE å·²å¼ƒç”¨ï¼Œä½¿ç”¨ get_last_lr() æ›¿ä»£

# ==========================================
# æ—©åœæœºåˆ¶é…ç½®
# ==========================================
EARLY_STOPPING_PATIENCE = 50  # è¿ç»­å¤šå°‘ä¸ªepochæ— æ”¹å–„å°±åœæ­¢è®­ç»ƒ
EARLY_STOPPING_MIN_DELTA = 1e-5  # æœ€å°æ”¹å–„é˜ˆå€¼ (0.001e-2 = 1e-5)
EARLY_STOPPING_START_EPOCH = 10  # ä»ç¬¬å‡ ä¸ªepochå¼€å§‹æ£€æŸ¥æ—©åœï¼ˆç»™æ¨¡å‹é¢„çƒ­æ—¶é—´ï¼‰
# DP æ¨¡å¼ä¸‹ batch_size æ˜¯æ‰€æœ‰å¡çš„æ€»å’Œ
# ä¾‹å¦‚ï¼š3å¼ å¡ï¼Œbatch_size=48 -> æ¯å¼ å¡åˆ†åˆ° 16 ä¸ªå›¾
# å¢åŠ  batch size ä»¥æé«˜ GPU åˆ©ç”¨ç‡
MATRIX_CACHE = {}
# ä¼˜åŒ–ï¼šDDPæ¨¡å¼ä¸‹æ¯ä¸ªGPUçš„batch size
# DDPæ€§èƒ½ä¼˜åŒ–ï¼šå¢å¤§batch_sizeä»¥æ›´å¥½åœ°åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—
# - 24GB æ˜¾å­˜ï¼šå»ºè®® 128-256 per GPU
# - 48GB æ˜¾å­˜ï¼šå»ºè®® 256-512 per GPU
# - å¦‚æœé‡åˆ°OOMï¼Œå¯ä»¥é€‚å½“å‡å°
TOTAL_BATCH_SIZE = 64  
EPOCH_ADAM = 2000  # åªä½¿ç”¨Adamä¼˜åŒ–å™¨
TOTAL_EPOCHS = EPOCH_ADAM
EPOCH_PRINT = 50
# ä»å…¨å±€é…ç½®æ–‡ä»¶å¯¼å…¥å‚æ•°
from config import N_ITER, SAVE_DIR, OUTPUT_DIR, LOSS_TYPE, MASTER_PORT, LOAD_PRETRAINED_MODEL, PRETRAINED_MODEL_DIR

# ==========================================
# 1.5. è¾“å‡ºç›®å½•é…ç½®
# ==========================================
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ç§»é™¤æ¨¡å—çº§åˆ«çš„æ‰“å°ï¼Œé¿å…DDPé‡å¤æ‰“å°

LR = 0.001

# æ€§èƒ½ä¼˜åŒ–é…ç½®
USE_AMP = False  # ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä½¿ç”¨çº¯float32ä»¥ä¿è¯ç¨³å®šæ€§ï¼‰
# torch.compile é…ç½®è¯´æ˜ï¼š
# - éœ€è¦ PyTorch 2.0+ æ”¯æŒ
# - å¯èƒ½æå‡ 10-20% å‰å‘ä¼ æ’­é€Ÿåº¦
# - é¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰
# - å¯èƒ½ä¸ DataParallel å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æœ‰å…¼å®¹æ€§é—®é¢˜
# - å»ºè®®ï¼šå…ˆæµ‹è¯•æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œå¦‚æœé‡åˆ°é”™è¯¯å¯ä»¥ç¦ç”¨
USE_COMPILE = False # å¯ç”¨torch.compileå¯æå‡10-20%é€Ÿåº¦ï¼ˆéœ€è¦PyTorch 2.0+ï¼‰
PIN_MEMORY = True  # å¯ç”¨pin_memoryä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“
USE_GRADIENT_CHECKPOINTING = False  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜ï¼ˆä¼šç¨å¾®é™ä½é€Ÿåº¦ï¼Œçº¦20-30%ï¼‰

# ==========================================
# 3. å…¨å±€çŸ©é˜µç¼“å­˜ (Multi-Device Matrix Cache)
# ==========================================
# ç”¨äºè§£å†³ DP æ¨¡å¼ä¸‹ä¸åŒæ˜¾å¡éœ€è¦è®¿é—®ä¸åŒè®¾å¤‡ä¸ŠçŸ©é˜µçš„é—®é¢˜

def load_matrix_to_cache(data_mapping, n_total, device_ids, is_main_process=True):
    """
    å°†æ‰€æœ‰çŸ©é˜µé¢„åŠ è½½åˆ° CPU ä¸Šï¼ˆä½¿ç”¨ pin_memory åŠ é€Ÿåç»­ä¼ è¾“ï¼‰ã€‚
    ç»“æ„: MATRIX_CACHE[k_idx] = (A_cpu, b_cpu)

    ä¼˜åŒ–ï¼šçŸ©é˜µå­˜å‚¨åœ¨ CPUï¼Œéœ€è¦æ—¶æ‰è½¬ç§»åˆ° GPUï¼Œé‡Šæ”¾æ˜¾å­˜ç©ºé—´ã€‚
    ä½¿ç”¨ pin_memory() åŠ é€Ÿ CPU åˆ° GPU çš„ä¼ è¾“ã€‚

    æ³¨æ„: device_ids å‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸å†ä¸ºæ¯ä¸ª GPU åˆ›å»ºå‰¯æœ¬
    """
    if is_main_process:
        print(f"æ­£åœ¨å°†çŸ©é˜µé¢„åŠ è½½åˆ° CPUï¼ˆä½¿ç”¨ pin_memory ä¼˜åŒ–ï¼‰...")
        print(f"   çŸ©é˜µå°†åœ¨éœ€è¦æ—¶åŠ¨æ€ä¼ è¾“åˆ° GPUï¼Œä»¥é‡Šæ”¾æ˜¾å­˜ç©ºé—´")
    valid_indices = [k for k in range(1, n_total + 1) if k in data_mapping]
    
    for idx, k_idx in enumerate(valid_indices):
        folder_path, folder_num, data_id = data_mapping[k_idx]
        try:
            # è¯»å–åŸå§‹æ•°æ® (CPU)
            Aij = load_file_data(folder_path, "Aij", folder_num, data_id)
            Av = load_file_data(folder_path, "Av", folder_num, data_id)
            b_data = load_file_data(folder_path, "b", folder_num, data_id)
            
            rows = Aij[:, 0].astype(int) - 1
            cols = Aij[:, 1].astype(int) - 1
            values = Av[:, 0] + 1j * Av[:, 1]
            N_nodes = len(b_data)
            b_val = b_data[:, 0] + 1j * b_data[:, 1]
            
            shape = (N_nodes, N_nodes)
            
            # åœ¨ CPU ä¸Šåˆ›å»ºå¼ é‡ï¼Œå¹¶ä½¿ç”¨ pin_memory åŠ é€Ÿåç»­ä¼ è¾“
            i = torch.from_numpy(np.vstack((rows, cols))).long()
            # v = torch.from_numpy(values.astype(np.complex128))  # ä½¿ç”¨åŒç²¾åº¦complex
            # b_k = torch.from_numpy(b_val.astype(np.complex128))  # ä½¿ç”¨åŒç²¾åº¦complex
            # æ”¹ä¸ºï¼š
            v = torch.from_numpy(values.astype(np.complex64))  # 32ä½å¤æ•°
            b_k = torch.from_numpy(b_val.astype(np.complex64))

            # # å°†AçŸ©é˜µå’Œbå‘é‡éƒ½æ”¾å¤§10å€
            # v = v * 10
            # b_k = b_k * 10

            # ä½¿ç”¨ pin_memory() å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­ï¼ŒåŠ é€Ÿ CPU->GPU ä¼ è¾“
            # æ³¨æ„ï¼šç¨€ç–çŸ©é˜µçš„ indices å’Œ values å¯ä»¥ pin_memory
            i = i.pin_memory()
            v = v.pin_memory()
            b_k = b_k.pin_memory()
            
            # åœ¨ CPU ä¸Šåˆ›å»ºç¨€ç–çŸ©é˜µï¼ˆä¸ coalesceï¼Œå»¶è¿Ÿåˆ° GPU ä¼ è¾“æ—¶ï¼‰
            # å­˜å‚¨ indices å’Œ valuesï¼ˆå·² pin_memoryï¼‰ä»¥åŠ shapeï¼Œè€Œä¸æ˜¯å®Œæ•´çš„ç¨€ç–çŸ©é˜µ
            # è¿™æ ·å¯ä»¥ä¿æŒ pin_memory çŠ¶æ€
            MATRIX_CACHE[k_idx] = {
                'indices': i,
                'values': v,
                'shape': shape,
                'b': b_k
            }
                
            if is_main_process and (idx + 1) % 1000 == 0:
                print(f"  å·²ç¼“å­˜ {idx + 1}/{len(valid_indices)} ä¸ªæ ·æœ¬")

        except Exception as e:
            if is_main_process:
                print(f"åŠ è½½æ ·æœ¬ {k_idx} å‡ºé”™: {e}")
            continue
    if is_main_process:
        print("çŸ©é˜µç¼“å­˜å®Œæˆï¼ˆå­˜å‚¨åœ¨ CPUï¼Œä½¿ç”¨ pin_memory ä¼˜åŒ–ï¼ŒåŒç²¾åº¦complex128ï¼‰ã€‚")
        # éªŒè¯ç²¾åº¦è®¾ç½®
        if MATRIX_CACHE:
            sample_key = list(MATRIX_CACHE.keys())[0]
            sample_data = MATRIX_CACHE[sample_key]
            print(f"ç¤ºä¾‹çŸ©é˜µç²¾åº¦æ£€æŸ¥: values.dtype={sample_data['values'].dtype}, b.dtype={sample_data['b'].dtype}")

def get_Ab(k_idx, device, dtype=None):
    """
    ä»ç¼“å­˜ä¸­è·å–å½“å‰è®¾å¤‡å¯¹åº”çš„ A å’Œ bã€‚
    å¦‚æœçŸ©é˜µåœ¨ CPU ä¸Šï¼Œåˆ™åŠ¨æ€ä¼ è¾“åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆä½¿ç”¨ non_blocking åŠ é€Ÿï¼‰ã€‚

    Args:
        k_idx: æ ·æœ¬ç´¢å¼•
        device: ç›®æ ‡è®¾å¤‡
        dtype: ç›®æ ‡æ•°æ®ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä¿æŒåŸæœ‰ç²¾åº¦

    ä¼˜åŒ–ï¼š
    1. çŸ©é˜µå­˜å‚¨åœ¨ CPUï¼Œéœ€è¦æ—¶æ‰ä¼ è¾“åˆ° GPU
    2. ä½¿ç”¨ non_blocking=True è¿›è¡Œå¼‚æ­¥ä¼ è¾“
    3. ä½¿ç”¨ pin_memory åŠ é€Ÿä¼ è¾“
    """
    if k_idx not in MATRIX_CACHE:
        raise RuntimeError(f"æœªæ‰¾åˆ°æ ·æœ¬ k={k_idx} çš„ç¼“å­˜æ•°æ®")
    
    cache_data = MATRIX_CACHE[k_idx]
    
    # å¦‚æœç›®æ ‡è®¾å¤‡æ˜¯ CPUï¼Œåœ¨ CPU ä¸Šæ„å»ºç¨€ç–çŸ©é˜µ
    if device.type == 'cpu':
        indices = cache_data['indices']
        values = cache_data['values']
        shape = cache_data['shape']
        b_cpu = cache_data['b']

        # æ ¹æ®éœ€è¦è½¬æ¢æ•°æ®ç±»å‹
        if dtype is not None:
            values = values.to(dtype)
            b_cpu = b_cpu.to(dtype)

        A_cpu = torch.sparse_coo_tensor(indices, values, shape, device=torch.device('cpu'))
        return A_cpu, b_cpu
    
    # å¦‚æœç›®æ ‡è®¾å¤‡æ˜¯ GPUï¼Œå°†çŸ©é˜µä¼ è¾“åˆ° GPU
    # ä½¿ç”¨ non_blocking=True è¿›è¡Œå¼‚æ­¥ä¼ è¾“ï¼ˆéœ€è¦ pin_memoryï¼‰
    try:
        # è·å–å·² pin_memory çš„ indices å’Œ values
        indices_cpu = cache_data['indices']
        values_cpu = cache_data['values']
        shape = cache_data['shape']
        b_cpu = cache_data['b']

        # æ ¹æ®éœ€è¦è½¬æ¢æ•°æ®ç±»å‹
        if dtype is not None:
            values_cpu = values_cpu.to(dtype)
            b_cpu = b_cpu.to(dtype)

        # å¼‚æ­¥ä¼ è¾“åˆ° GPUï¼ˆnon_blocking éœ€è¦ pin_memoryï¼‰
        indices_gpu = indices_cpu.to(device, non_blocking=True)
        values_gpu = values_cpu.to(device, non_blocking=True)

        # åœ¨ GPU ä¸Šé‡å»ºç¨€ç–çŸ©é˜µå¹¶ coalesce
        A_gpu = torch.sparse_coo_tensor(indices_gpu, values_gpu, shape, device=device).coalesce()

        # å¼‚æ­¥ä¼ è¾“ b å‘é‡
        b_gpu = b_cpu.to(device, non_blocking=True)

        return A_gpu, b_gpu
    except Exception as e:
        # å¦‚æœå¼‚æ­¥ä¼ è¾“å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥ä¼ è¾“
        if is_main_process():
            print(f"è­¦å‘Š: å¼‚æ­¥ä¼ è¾“å¤±è´¥ï¼Œä½¿ç”¨åŒæ­¥ä¼ è¾“ (k={k_idx}): {e}")
        indices_cpu = cache_data['indices']
        values_cpu = cache_data['values']
        shape = cache_data['shape']
        b_cpu = cache_data['b']

        # æ ¹æ®éœ€è¦è½¬æ¢æ•°æ®ç±»å‹
        if dtype is not None:
            values_cpu = values_cpu.to(dtype)
            b_cpu = b_cpu.to(dtype)

        indices_gpu = indices_cpu.to(device)
        values_gpu = values_cpu.to(device)
        A_gpu = torch.sparse_coo_tensor(indices_gpu, values_gpu, shape, device=device).coalesce()
        b_gpu = b_cpu.to(device)

        return A_gpu, b_gpu

# ==========================================
# 3. è¾…åŠ©å‡½æ•°ï¼šç»˜åˆ¶è®­ç»ƒæ›²çº¿
# ==========================================
def plot_training_curve(train_losses=None, test_losses=None, save_path="training_curve.svg", data_file=None):
    """
    ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†losså˜åŒ–æ›²çº¿å¹¶ä¿å­˜ä¸ºæ–‡ä»¶

    Args:
        train_losses: è®­ç»ƒé›†lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        test_losses: æµ‹è¯•é›†lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        save_path: ä¿å­˜è·¯å¾„
        data_file: è®­ç»ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»æ–‡ä»¶åŠ è½½æ•°æ®
    """
    # åœ¨DDPç¯å¢ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œç»˜å›¾
    if not is_main_process():
        return
    if data_file is not None:
        # ä»æ–‡ä»¶åŠ è½½æ•°æ®
        data = load_training_data(data_file)
        if data is None:
            if is_main_process():
                print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
            return
        train_losses = data.get('train_losses', [])
        test_losses = data.get('test_losses', [])
        if not train_losses or not test_losses:
            if is_main_process():
                print("âŒ è®­ç»ƒæ•°æ®ä¸­ç¼ºå°‘lossä¿¡æ¯ï¼Œè·³è¿‡ç»˜å›¾")
            return
    plt.figure(figsize=(12, 6))

    epochs = range(1, len(train_losses) + 1)

    # ç»˜åˆ¶è®­ç»ƒloss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦

    # ç»˜åˆ¶æµ‹è¯•loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, test_losses, 'r-', label='Test Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Test Loss Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦

    # ä¿å­˜ä¸ºçŸ¢é‡å›¾æ ¼å¼
    # PDFæ ¼å¼ï¼ˆé«˜è´¨é‡æ‰“å°ï¼‰
    pdf_path = os.path.join(os.path.dirname(save_path), os.path.basename(save_path).replace('.png', '.pdf'))
    plt.savefig(pdf_path, bbox_inches='tight')
    plt.close()

    if is_main_process():
        print(f"âœ… è®­ç»ƒæ›²çº¿PDFå·²ä¿å­˜åˆ°: {pdf_path}")

    # SVGæ ¼å¼ï¼ˆç½‘é¡µå’Œç°ä»£åº”ç”¨ï¼‰
    plt.figure(figsize=(12, 6))

    # é‡æ–°ç»˜åˆ¶ï¼ˆåˆå¹¶åœ¨ä¸€ä¸ªå›¾ä¸­ï¼‰
    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(epochs, test_losses, 'r-', label='Test Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Test Loss Curves')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦

    svg_path = os.path.join(os.path.dirname(save_path), os.path.basename(save_path).replace('.png', '.svg'))
    plt.savefig(svg_path, bbox_inches='tight')
    plt.close()

    if is_main_process():
        print(f"âœ… è®­ç»ƒæ›²çº¿SVGå·²ä¿å­˜åˆ°: {svg_path}")


def plot_mse_res_loss(train_mse=None, test_mse=None, train_res=None, test_res=None, save_path="mse_res_loss.svg", data_file=None):
    """
    ç»˜åˆ¶åŒ…å«MSE losså’ŒRES lossçš„è®­ç»ƒæ›²çº¿

    Args:
        train_mse: è®­ç»ƒé›†MSE lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        test_mse: æµ‹è¯•é›†MSE lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        train_res: è®­ç»ƒé›†RES lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        test_res: æµ‹è¯•é›†RES lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        save_path: ä¿å­˜è·¯å¾„
        data_file: è®­ç»ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»æ–‡ä»¶åŠ è½½æ•°æ®
    """
    # åœ¨DDPç¯å¢ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œç»˜å›¾
    if not is_main_process():
        return
    if data_file is not None:
        # ä»æ–‡ä»¶åŠ è½½æ•°æ®
        data = load_training_data(data_file)
        if data is None:
            if is_main_process():
                print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
            return
        train_mse = data.get('train_mse_losses', [])
        test_mse = data.get('test_mse_losses', [])
        train_res = data.get('train_res_losses', [])
        test_res = data.get('test_res_losses', [])
        if not all([train_mse, test_mse, train_res, test_res]):
            if is_main_process():
                print("âŒ è®­ç»ƒæ•°æ®ä¸­ç¼ºå°‘MSEæˆ–RES lossä¿¡æ¯ï¼Œè·³è¿‡ç»˜å›¾")
            return
    plt.figure(figsize=(14, 6))
    
    epochs = range(1, len(train_mse) + 1)
    
    # ç»˜åˆ¶MSE loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_mse, 'b-', label='Training MSE', linewidth=2, marker='o', markersize=3)
    plt.plot(epochs, test_mse, 'r-', label='Testing MSE', linewidth=2, marker='+', markersize=3)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss (log scale)')
    plt.title('MSE Loss Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    # ç»˜åˆ¶RES loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_res, 'b-', label='Training Res', linewidth=2, marker='o', markersize=3)
    plt.plot(epochs, test_res, 'r-', label='Testing Res', linewidth=2, marker='+', markersize=3)
    plt.xlabel('Epoch')
    plt.ylabel('RES loss')
    plt.title('Residual Loss Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 0.2)  # è®¾ç½®yè½´èŒƒå›´ä¸º0~0.2
    
    # ä¿å­˜ä¸ºçŸ¢é‡å›¾æ ¼å¼ (SVG)
    svg_path = os.path.join(os.path.dirname(save_path), os.path.basename(save_path).replace('.png', '.svg'))
    plt.savefig(svg_path, bbox_inches='tight')
    plt.close()

    if is_main_process():
        print(f"âœ… MSEå’ŒRES lossæ›²çº¿SVGå·²ä¿å­˜åˆ°: {svg_path}")


def plot_mse_loss_distribution(train_mse_losses=None, test_mse_losses=None, save_path="mse_loss_distribution.svg", data_file=None):
    """
    ç»˜åˆ¶MSE lossåˆ†å¸ƒæŸ±çŠ¶å›¾

    Args:
        train_mse_losses: è®­ç»ƒé›†MSE lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        test_mse_losses: æµ‹è¯•é›†MSE lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›data_fileåˆ™å¿½ç•¥ï¼‰
        save_path: ä¿å­˜è·¯å¾„
        data_file: è®­ç»ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»æ–‡ä»¶åŠ è½½æ•°æ®
    """
    # åœ¨DDPç¯å¢ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œç»˜å›¾
    if not is_main_process():
        return
    if data_file is not None:
        # ä»æ–‡ä»¶åŠ è½½æ•°æ®
        data = load_training_data(data_file)
        if data is None:
            if is_main_process():
                print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
            return
        train_mse_losses = data.get('train_mse_losses', [])
        test_mse_losses = data.get('test_mse_losses', [])
        if not train_mse_losses or not test_mse_losses:
            if is_main_process():
                print("âŒ è®­ç»ƒæ•°æ®ä¸­ç¼ºå°‘MSE lossä¿¡æ¯ï¼Œè·³è¿‡ç»˜å›¾")
            return
    plt.figure(figsize=(12, 6))

    # ç¡®ä¿è¾“å…¥æ˜¯æœ‰æ•ˆçš„æ•°å€¼åˆ—è¡¨
    if not train_mse_losses or not test_mse_losses:
        if is_main_process():
            print("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†æˆ–æµ‹è¯•é›†MSE lossæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç»˜å›¾")
        plt.close()
        return

    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿‡æ»¤æ— æ•ˆå€¼
    train_mse_losses = np.array(train_mse_losses, dtype=np.float32)
    test_mse_losses = np.array(test_mse_losses, dtype=np.float32)

    # è¿‡æ»¤æ‰NaNå’Œinfå€¼
    train_mse_losses = train_mse_losses[np.isfinite(train_mse_losses)]
    test_mse_losses = test_mse_losses[np.isfinite(test_mse_losses)]

    if len(train_mse_losses) == 0 or len(test_mse_losses) == 0:
        if is_main_process():
            print("âš ï¸  è­¦å‘Šï¼šè¿‡æ»¤åè®­ç»ƒé›†æˆ–æµ‹è¯•é›†MSE lossæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç»˜å›¾")
        plt.close()
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    train_mean = np.mean(train_mse_losses)
    train_std = np.std(train_mse_losses)
    test_mean = np.mean(test_mse_losses)
    test_std = np.std(test_mse_losses)

    # è®¾ç½®binsï¼ˆå¯¹æ•°åæ ‡éœ€è¦ç‰¹æ®Šçš„å¤„ç†ï¼‰
    all_mse_losses = np.concatenate([train_mse_losses, test_mse_losses])
    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ­£æ•°ï¼ˆMSE lossåº”è¯¥éƒ½æ˜¯æ­£æ•°ï¼‰
    all_mse_losses = all_mse_losses[all_mse_losses > 0]
    if len(all_mse_losses) == 0:
        if is_main_process():
            print("âš ï¸  è­¦å‘Šï¼šæ‰€æœ‰MSE losså€¼éƒ½æ˜¯éæ­£æ•°ï¼Œè·³è¿‡ç»˜å›¾")
        plt.close()
        return

    # åˆ›å»ºå¯¹æ•°bins
    log_min = np.log10(max(all_mse_losses.min(), 1e-10))  # é¿å…log(0)
    log_max = np.log10(all_mse_losses.max())
    bins = np.logspace(log_min, log_max, 50)

    # è®¡ç®—ç›´æ–¹å›¾æ•°æ®
    train_hist, _ = np.histogram(train_mse_losses, bins=bins, density=True)
    test_hist, _ = np.histogram(test_mse_losses, bins=bins, density=True)

    # è®¡ç®—binä¸­å¿ƒç”¨äºç»˜åˆ¶
    bin_centers = (bins[:-1] + bins[1:]) / 2

    # è®¾ç½®barå®½åº¦
    bar_width = np.diff(bins) * 0.8  # æ¯ä¸ªbinçš„80%å®½åº¦

    # ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æŸ±çŠ¶å›¾ï¼ˆåˆ†å¼€æ”¾ç½®ï¼‰
    plt.bar(bin_centers - bar_width/4, train_hist, width=bar_width/2, alpha=0.8, color='coral',
            label=f'Training (Î¼={train_mean:.3f}, Ïƒ={train_std:.5f})', edgecolor='black', linewidth=0.5)

    plt.bar(bin_centers + bar_width/4, test_hist, width=bar_width/2, alpha=0.8, color='cyan',
            label=f'Testing (Î¼={test_mean:.3f}, Ïƒ={test_std:.5f})', edgecolor='black', linewidth=0.5)

    # æ‹Ÿåˆé«˜æ–¯åˆ†å¸ƒå¹¶ç»˜åˆ¶
    from scipy import stats

    # è®­ç»ƒé›†é«˜æ–¯æ‹Ÿåˆ
    train_params = stats.norm.fit(train_mse_losses)
    train_x = np.logspace(log_min, log_max, 100)  # åœ¨å¯¹æ•°ç©ºé—´å‡åŒ€åˆ†å¸ƒçš„ç‚¹ç”¨äºæ˜¾ç¤º
    train_pdf = stats.norm.pdf(train_x, *train_params)
    plt.plot(train_x, train_pdf, 'r-', linewidth=2, label='Training Gaussian Fit')

    # æµ‹è¯•é›†é«˜æ–¯æ‹Ÿåˆ
    test_params = stats.norm.fit(test_mse_losses)
    test_x = np.logspace(log_min, log_max, 100)  # åœ¨å¯¹æ•°ç©ºé—´å‡åŒ€åˆ†å¸ƒçš„ç‚¹ç”¨äºæ˜¾ç¤º
    test_pdf = stats.norm.pdf(test_x, *test_params)
    plt.plot(test_x, test_pdf, 'b-', linewidth=2, label='Testing Gaussian Fit')

    plt.xlabel('MSE Loss (log scale)')
    plt.ylabel('Density')
    plt.title('MSE Loss Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log')  # è®¾ç½®xè½´ä¸ºå¯¹æ•°åæ ‡
    
    # ä¿å­˜ä¸ºçŸ¢é‡å›¾æ ¼å¼ (SVG)
    svg_path = os.path.join(os.path.dirname(save_path), os.path.basename(save_path).replace('.png', '.svg'))
    plt.savefig(svg_path, bbox_inches='tight')
    plt.close()

    if is_main_process():
        print(f"âœ… MSE lossåˆ†å¸ƒå›¾SVGå·²ä¿å­˜åˆ°: {svg_path}")
        print(f"   è®­ç»ƒé›†: å‡å€¼={train_mean:.6f}, æ ‡å‡†å·®={train_std:.6f}")
        print(f"   æµ‹è¯•é›†: å‡å€¼={test_mean:.6f}, æ ‡å‡†å·®={test_std:.6f}")


def compute_relative_errors(solver, data_loader, data_mapping, device, matrix_dtype):
    """
    è®¡ç®—æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„ç›¸å¯¹è¯¯å·®
    
    Args:
        solver: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        data_mapping: æ•°æ®æ˜ å°„å­—å…¸
        device: è®¾å¤‡
        matrix_dtype: çŸ©é˜µæ•°æ®ç±»å‹
    
    Returns:
        relative_errors: æ¯ä¸ªæ ·æœ¬çš„ç›¸å¯¹è¯¯å·®åˆ—è¡¨
    """
    solver.eval()
    relative_errors = []
    
    with torch.no_grad():
        for data_list in data_loader:
            # è·å–batch
            if not isinstance(data_list, list):
                data_list = [data_list]
            
            processed_data_list = []
            for item in data_list:
                if isinstance(item, Data):
                    if not hasattr(item, 'k_idx'):
                        item.k_idx = torch.tensor([0])
                    elif not isinstance(item.k_idx, torch.Tensor):
                        item.k_idx = torch.tensor([item.k_idx] if not isinstance(item.k_idx, (list, tuple)) else item.k_idx)
                    processed_data_list.append(item)
                elif isinstance(item, tuple) and len(item) > 0 and isinstance(item[0], Data):
                    processed_data_list.append(item[0])
            
            try:
                batch = Batch.from_data_list(processed_data_list)
            except:
                from torch_geometric.data.collate import collate
                batch, _, _ = collate(Data, processed_data_list, increment=True, add_batch=True, follow_batch=[])
            
            batch = batch.to(device)
            k_all = batch.k_idx
            node_batch = batch.batch
            B = k_all.size(0)
            
            # è·å–æ¨¡å‹ç²¾åº¦ - å…¼å®¹ä¸åŒå±‚ç±»å‹
            raw_model = solver.module if isinstance(solver, (DataParallel, DDP)) else solver
            if hasattr(raw_model.model_real.networks[0].gcn1, 'conv'):
                is_double_precision = raw_model.model_real.networks[0].gcn1.conv.lin_fusion.weight.dtype == torch.float64
            elif hasattr(raw_model.model_real.networks[0].gcn1, 'spatial_conv'):
                is_double_precision = raw_model.model_real.networks[0].gcn1.spatial_conv.lin_fusion.weight.dtype == torch.float64
            elif hasattr(raw_model.model_real.networks[0].gcn1, 'linear'):
                is_double_precision = raw_model.model_real.networks[0].gcn1.linear.weight.dtype == torch.float64
            else:
                is_double_precision = False

            if is_double_precision:
                data_dtype = torch.float64
            else:
                data_dtype = torch.float32
            
            # å‡†å¤‡æ•°æ®
            eps_feat = batch.x[:, 0:2].to(data_dtype)
            current_E_real = batch.x[:, 4].to(data_dtype)
            current_E_imag = batch.x[:, 5].to(data_dtype)
            true_real = batch.y[:, 0].to(data_dtype)
            true_imag = batch.y[:, 1].to(data_dtype)
            
            # åŠ è½½çŸ©é˜µ
            A_list = []
            b_list = []
            for b_idx in range(B):
                k = int(k_all[b_idx].item())
                A, b = get_Ab(k, device, matrix_dtype)
                A_list.append(A)
                b_list.append(b)
            
            # å‰å‘ä¼ æ’­
            E_real_cur = current_E_real
            E_imag_cur = current_E_imag
            
            for iter_idx in range(raw_model.n_iter):
                # è®¡ç®—æ®‹å·®
                r_real_list = []
                r_imag_list = []
                for b_idx in range(B):
                    mask = (node_batch == b_idx)
                    E_r = E_real_cur[mask]
                    E_i = E_imag_cur[mask]
                    A = A_list[b_idx]
                    b_vec = b_list[b_idx]
                    E_c = torch.complex(E_r, E_i)
                    Ax = torch.sparse.mm(A, E_c.unsqueeze(-1)).squeeze(-1)
                    r_c = b_vec - Ax
                    if data_dtype == torch.float64:
                        r_real_list.append(r_c.real.double())
                        r_imag_list.append(r_c.imag.double())
                    else:
                        r_real_list.append(r_c.real.float())
                        r_imag_list.append(r_c.imag.float())
                    del E_c, Ax, r_c
                
                r_real = torch.cat(r_real_list, dim=0)
                r_imag = torch.cat(r_imag_list, dim=0)
                del r_real_list, r_imag_list
                
                # GNNå‰å‘
                # ä»batch.xä¸­æå–èƒŒæ™¯åœº
                bg_real = batch.x[:, 6].to(data_dtype)
                bg_imag = batch.x[:, 7].to(data_dtype)
                x_in = torch.cat([
                    eps_feat,
                    r_real.view(-1, 1),
                    r_imag.view(-1, 1),
                    E_real_cur.view(-1, 1),  # å½“å‰ç”µåœºå®éƒ¨ (éšè¿­ä»£æ›´æ–°)
                    E_imag_cur.view(-1, 1),  # å½“å‰ç”µåœºè™šéƒ¨ (éšè¿­ä»£æ›´æ–°)
                    bg_real.view(-1, 1),     # èƒŒæ™¯åœºå®éƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
                    bg_imag.view(-1, 1)      # èƒŒæ™¯åœºè™šéƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
                ], dim=1)

                delta_real = raw_model.model_real(x_in, batch.edge_index, batch.batch, iter_idx)
                delta_imag = raw_model.model_imag(x_in, batch.edge_index, batch.batch, iter_idx)
                
                E_real_cur = E_real_cur + delta_real.view(-1)
                E_imag_cur = E_imag_cur + delta_imag.view(-1)
                del x_in, delta_real, delta_imag, r_real, r_imag
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç›¸å¯¹è¯¯å·®
            for b_idx in range(B):
                mask = (node_batch == b_idx)
                pred_real = E_real_cur[mask]
                pred_imag = E_imag_cur[mask]
                true_r = true_real[mask]
                true_i = true_imag[mask]
                
                # è®¡ç®—å¹³æ–¹çš„ç›¸å¯¹è¯¯å·®: ||pred - true||_2^2 / ||true||_2^2ï¼ˆä¸å…¬å¼ä¸€è‡´ï¼‰
                # åˆ†å­ï¼š||pred - true||_2^2
                numerator = ((pred_real - true_r).pow(2).sum() + (pred_imag - true_i).pow(2).sum())
                # åˆ†æ¯ï¼š||true||_2^2
                denominator = (true_r.pow(2).sum() + true_i.pow(2).sum())
                
                if denominator > 1e-10:  # é¿å…é™¤é›¶
                    # å¹³æ–¹çš„ç›¸å¯¹è¯¯å·®ï¼š||x^FEM - x^GCN||_2^2 / ||x^FEM||_2^2
                    rel_error_squared = (numerator / denominator).item()
                    relative_errors.append(rel_error_squared)
            
            del A_list, b_list, E_real_cur, E_imag_cur
    
    return relative_errors

def save_training_data(training_data, save_path="training_data.json"):
    """
    ä¿å­˜è®­ç»ƒæ•°æ®åˆ°JSONæ–‡ä»¶

    Args:
        training_data: åŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
        save_path: ä¿å­˜è·¯å¾„
    """
    # åœ¨DDPç¯å¢ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸­ä¿å­˜æ•°æ®
    if not is_main_process():
        return
    import json

    # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿JSONåºåˆ—åŒ–
    def convert_to_serializable(obj):
        if isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, (int, float, str, bool)) or obj is None:
            return obj
        else:
            return str(obj)

    serializable_data = convert_to_serializable(training_data)

    with open(save_path, 'w') as f:
        json.dump(serializable_data, f, indent=2)

    if is_main_process():
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {save_path}")

def load_training_data(load_path="training_data.json"):
    """
    ä»JSONæ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®

    Args:
        load_path: åŠ è½½è·¯å¾„

    Returns:
        dict: è®­ç»ƒæ•°æ®å­—å…¸
    """
    import json

    try:
        with open(load_path, 'r') as f:
            data = json.load(f)
        if is_main_process():
            print(f"âœ… è®­ç»ƒæ•°æ®å·²ä» {load_path} åŠ è½½")
        return data
    except FileNotFoundError:
        if is_main_process():
            print(f"âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ {load_path}")
        return None
    except Exception as e:
        if is_main_process():
            print(f"âŒ åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        return None

# ==========================================
# 3. è¾…åŠ©å‡½æ•°ï¼šå¤„ç† DataParallel çš„è¾“å‡º
# ==========================================
def extract_loss_and_num_nodes(outputs):
    """
    ä» DataParallel çš„è¾“å‡ºä¸­æå– loss å’Œ num_nodesã€‚

    æ³¨æ„ï¼šforward æ–¹æ³•è¿”å›å•ä¸ª tensorï¼š
    - æ™®é€šæ¨¡å¼ï¼š[loss_sum, res_sum, num_nodes, num_samples] (4ä¸ªå€¼)
    - Hybridæ¨¡å¼ï¼š[loss_sum, res_sum, num_nodes, num_samples, mse_loss_sum, phi_loss_sum] (6ä¸ªå€¼)

    Args:
        outputs: DataParallel è¿”å›çš„ç»“æœ
            - å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸Šè¿°æ ¼å¼çš„ tensor
            - å¦‚æœæ˜¯å•ä¸ª tensorï¼Œå½¢çŠ¶ä¸º [4]/[6] æˆ– [N, 4]/[N, 6]ï¼ˆN ä¸ª GPUï¼‰

    Returns:
        tuple: (total_loss_sum, total_res_sum, total_num_nodes, total_num_samples, total_mse_sum, total_phi_sum)
            - total_loss_sum: æ‰€æœ‰ GPU çš„ loss æ€»å’Œ (tensor)
            - total_res_sum: æ‰€æœ‰ GPU çš„ RES loss æ€»å’Œ (tensor)
            - total_num_nodes: æ‰€æœ‰ GPU çš„ num_nodes æ€»å’Œ (int)
            - total_num_samples: æ‰€æœ‰ GPU çš„ num_samples æ€»å’Œ (int)
            - total_mse_sum: æ‰€æœ‰ GPU çš„ MSE loss æ€»å’Œ (tensorï¼Œä»…åœ¨hybridæ¨¡å¼ä¸‹æœ‰æ•ˆ)
            - total_phi_sum: æ‰€æœ‰ GPU çš„ Phi loss æ€»å’Œ (tensorï¼Œä»…åœ¨hybridæ¨¡å¼ä¸‹æœ‰æ•ˆ)
    """
    # æ£€æŸ¥ outputs æ˜¯å¦ä¸ºç©ºï¼ˆåªæ£€æŸ¥ None å’Œç©ºåˆ—è¡¨ï¼Œä¸æ£€æŸ¥å¼ é‡ï¼‰
    if outputs is None:
        return torch.tensor(0.0), torch.tensor(0.0), 0, 0, torch.tensor(0.0), torch.tensor(0.0)
    if isinstance(outputs, list) and len(outputs) == 0:
        return torch.tensor(0.0), torch.tensor(0.0), 0, 0, torch.tensor(0.0), torch.tensor(0.0)

    # å¤„ç† DataParallel çš„è¾“å‡ºæ ¼å¼
    # DataParallel å¯èƒ½è¿”å›ï¼š
    # 1. åˆ—è¡¨: [[loss1, res1, n1, s1, mse1, phi1], [loss2, res2, n2, s2, mse2, phi2], ...] (hybridæ¨¡å¼)
    # 2. åˆ—è¡¨: [[loss1, res1, n1, s1], [loss2, res2, n2, s2], ...] (æ™®é€šæ¨¡å¼)
    # 3. å•ä¸ª tensor: [[loss1, res1, n1, s1, mse1, phi1], ...] å½¢çŠ¶ä¸º [N, 6] æˆ– [N, 4]
    # 4. å•ä¸ª tensor: [loss, res, n, s, mse, phi] å½¢çŠ¶ä¸º [6] æˆ– [4]ï¼ˆå• GPU æƒ…å†µï¼‰

    if isinstance(outputs, list):
        # åˆ—è¡¨æ ¼å¼ï¼šæ¯ä¸ªå…ƒç´ æ˜¯ [loss, res, num_nodes, num_samples, mse?, phi?] çš„ tensor
        loss_list = []
        res_list = []
        num_nodes_list = []
        num_samples_list = []
        mse_list = []
        phi_list = []
        is_hybrid_mode = False

        for o in outputs:
            if isinstance(o, torch.Tensor):
                if o.dim() == 1:
                    if o.shape[0] == 4:
                        # æ™®é€šæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [4] çš„ tensor
                        loss_list.append(o[0])
                        res_list.append(o[1])
                        num_nodes_list.append(o[2].item())
                        num_samples_list.append(o[3].item())
                        mse_list.append(torch.tensor(0.0))  # å ä½ç¬¦
                        phi_list.append(torch.tensor(0.0))  # å ä½ç¬¦
                    elif o.shape[0] == 6:
                        # Hybridæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [6] çš„ tensor
                        is_hybrid_mode = True
                        loss_list.append(o[0])
                        res_list.append(o[1])
                        num_nodes_list.append(o[2].item())
                        num_samples_list.append(o[3].item())
                        mse_list.append(o[4])
                        phi_list.append(o[5])
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {o.shape}")
                elif o.dim() == 2:
                    if o.shape[1] == 4:
                        # æ™®é€šæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [N, 4] çš„ tensorï¼ˆå¤šä¸ª GPU åˆå¹¶ï¼‰
                        loss_list.append(o[:, 0].sum())
                        res_list.append(o[:, 1].sum())
                        num_nodes_list.append(o[:, 2].sum().item())
                        num_samples_list.append(o[:, 3].sum().item())
                        mse_list.append(torch.tensor(0.0))  # å ä½ç¬¦
                        phi_list.append(torch.tensor(0.0))  # å ä½ç¬¦
                    elif o.shape[1] == 6:
                        # Hybridæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [N, 6] çš„ tensorï¼ˆå¤šä¸ª GPU åˆå¹¶ï¼‰
                        is_hybrid_mode = True
                        loss_list.append(o[:, 0].sum())
                        res_list.append(o[:, 1].sum())
                        num_nodes_list.append(o[:, 2].sum().item())
                        num_samples_list.append(o[:, 3].sum().item())
                        mse_list.append(o[:, 4].sum())
                        phi_list.append(o[:, 5].sum())
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {o.shape}")
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {o.shape}")
            else:
                raise TypeError(f"ä¸æ”¯æŒçš„è¾“å‡ºç±»å‹: {type(o)}")

        total_loss = torch.stack(loss_list).sum() if loss_list else torch.tensor(0.0)
        total_res = torch.stack(res_list).sum() if res_list else torch.tensor(0.0)
        total_num_nodes = sum(num_nodes_list)
        total_num_samples = sum(num_samples_list)
        total_mse = torch.stack(mse_list).sum() if is_hybrid_mode and mse_list else torch.tensor(0.0)
        total_phi = torch.stack(phi_list).sum() if is_hybrid_mode and phi_list else torch.tensor(0.0)
        
    elif isinstance(outputs, torch.Tensor):
        # å•ä¸ª tensor æ ¼å¼
        if outputs.dim() == 1:
            if outputs.shape[0] == 4:
                # æ™®é€šæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [4]ï¼šå• GPU
                total_loss = outputs[0]
                total_res = outputs[1]
                total_num_nodes = int(outputs[2].item())
                total_num_samples = int(outputs[3].item())
                total_mse = torch.tensor(0.0)
                total_phi = torch.tensor(0.0)
            elif outputs.shape[0] == 6:
                # Hybridæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [6]ï¼šå• GPU
                total_loss = outputs[0]
                total_res = outputs[1]
                total_num_nodes = int(outputs[2].item())
                total_num_samples = int(outputs[3].item())
                total_mse = outputs[4]
                total_phi = outputs[5]
            elif outputs.shape[0] % 4 == 0 and outputs.shape[0] % 6 != 0:
                # æ™®é€šæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [4*N]ï¼šå¤šä¸ª GPU çš„è¾“å‡ºè¢«å±•å¹³ï¼ˆä¾‹å¦‚ [12] = 3ä¸ªGPU * 4ï¼‰
                # éœ€è¦é‡å¡‘ä¸º [N, 4] æ ¼å¼
                n_gpus = outputs.shape[0] // 4
                outputs_reshaped = outputs.view(n_gpus, 4)
                total_loss = outputs_reshaped[:, 0].sum()
                total_res = outputs_reshaped[:, 1].sum()
                total_num_nodes = int(outputs_reshaped[:, 2].sum().item())
                total_num_samples = int(outputs_reshaped[:, 3].sum().item())
                total_mse = torch.tensor(0.0)
                total_phi = torch.tensor(0.0)
            elif outputs.shape[0] % 6 == 0:
                # Hybridæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [6*N]ï¼šå¤šä¸ª GPU çš„è¾“å‡ºè¢«å±•å¹³
                # éœ€è¦é‡å¡‘ä¸º [N, 6] æ ¼å¼
                n_gpus = outputs.shape[0] // 6
                outputs_reshaped = outputs.view(n_gpus, 6)
                total_loss = outputs_reshaped[:, 0].sum()
                total_res = outputs_reshaped[:, 1].sum()
                total_num_nodes = int(outputs_reshaped[:, 2].sum().item())
                total_num_samples = int(outputs_reshaped[:, 3].sum().item())
                total_mse = outputs_reshaped[:, 4].sum()
                total_phi = outputs_reshaped[:, 5].sum()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {outputs.shape}ï¼ˆä¸€ç»´å¼ é‡é•¿åº¦å¿…é¡»æ˜¯4æˆ–6çš„å€æ•°ï¼‰")
        elif outputs.dim() == 2:
            if outputs.shape[1] == 4:
                # æ™®é€šæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [N, 4]ï¼šå¤šä¸ª GPU åˆå¹¶
                total_loss = outputs[:, 0].sum()
                total_res = outputs[:, 1].sum()
                total_num_nodes = int(outputs[:, 2].sum().item())
                total_num_samples = int(outputs[:, 3].sum().item())
                total_mse = torch.tensor(0.0)
                total_phi = torch.tensor(0.0)
            elif outputs.shape[1] == 6:
                # Hybridæ¨¡å¼ï¼šå½¢çŠ¶ä¸º [N, 6]ï¼šå¤šä¸ª GPU åˆå¹¶
                total_loss = outputs[:, 0].sum()
                total_res = outputs[:, 1].sum()
                total_num_nodes = int(outputs[:, 2].sum().item())
                total_num_samples = int(outputs[:, 3].sum().item())
                total_mse = outputs[:, 4].sum()
                total_phi = outputs[:, 5].sum()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {outputs.shape}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ tensor å½¢çŠ¶: {outputs.shape}")
    else:
        raise TypeError(f"ä¸æ”¯æŒçš„è¾“å‡ºç±»å‹: {type(outputs)}")

    return total_loss, total_res, total_num_nodes, total_num_samples, total_mse, total_phi


# ==========================================
# 4. Lossè®¡ç®—å‡½æ•°
# ==========================================
def compute_mse_loss(E_real_cur, E_imag_cur, batch_y, num_nodes=None):
    """
    è®¡ç®—ä¼ ç»Ÿçš„MSEæŸå¤±ï¼š||pred - true||^2ï¼ˆè¿”å›æ€»å’Œï¼Œæœªå¹³å‡ï¼‰

    Args:
        E_real_cur: é¢„æµ‹çš„å®éƒ¨ç”µåœº [N]
        E_imag_cur: é¢„æµ‹çš„è™šéƒ¨ç”µåœº [N]
        batch_y: çœŸå®æ ‡ç­¾ [N, 2]
        num_nodes: èŠ‚ç‚¹æ€»æ•°ï¼ˆç”¨äºæ¥å£ç»Ÿä¸€ï¼Œä¸ä½¿ç”¨ï¼‰

    Returns:
        loss_sum: MSEæŸå¤±æ€»å’Œï¼ˆæœªå¹³å‡ï¼‰
    """
    true_real = batch_y[:, 0]
    true_imag = batch_y[:, 1]

    # è®¡ç®—å®éƒ¨å’Œè™šéƒ¨çš„å¹³æ–¹è¯¯å·®æ€»å’Œ
    term1 = (E_real_cur - true_real).pow(2).sum()  # å®éƒ¨è¯¯å·®å¹³æ–¹å’Œ
    term2 = (E_imag_cur - true_imag).pow(2).sum()  # è™šéƒ¨è¯¯å·®å¹³æ–¹å’Œ

    # è¿”å›æ€»SSEï¼ˆSum of Squared Errorsï¼‰
    loss_sum = 0.5 * (term1 + term2)

    # é‡Šæ”¾ä¸­é—´å˜é‡
    del term1, term2

    return loss_sum

def compute_phi_loss(E_real_cur, E_imag_cur, k_all, node_batch, B, device, matrix_dtype, num_nodes=None):
    """
    è®¡ç®—PhiæŸå¤±ï¼š||A*x - b||^2ï¼ˆç‰©ç†æ®‹å·®ï¼Œä¸éœ€è¦èŠ‚ç‚¹å¹³å‡ï¼‰

    Args:
        E_real_cur: é¢„æµ‹çš„å®éƒ¨ç”µåœº [N]
        E_imag_cur: é¢„æµ‹çš„è™šéƒ¨ç”µåœº [N]
        k_all: æ ·æœ¬ç´¢å¼• [B]
        node_batch: èŠ‚ç‚¹åˆ°æ‰¹æ¬¡çš„æ˜ å°„ [N]
        B: æ‰¹æ¬¡å¤§å°
        device: è®¡ç®—è®¾å¤‡
        matrix_dtype: çŸ©é˜µæ•°æ®ç±»å‹
        num_nodes: èŠ‚ç‚¹æ€»æ•°ï¼ˆPhi lossä¸ä½¿ç”¨ï¼Œç”¨äºæ¥å£ç»Ÿä¸€ï¼‰

    Returns:
        loss_sum: PhiæŸå¤±æ€»å’Œï¼ˆä¸è¿›è¡ŒèŠ‚ç‚¹å¹³å‡ï¼‰
    """
    phi_losses = []
    for b_idx in range(B):
        k = int(k_all[b_idx].item())
        A, b = get_Ab(k, device, matrix_dtype)

        mask = (node_batch == b_idx)
        E_r = E_real_cur[mask]
        E_i = E_imag_cur[mask]

        E_c = torch.complex(E_r, E_i)
        Ax = torch.sparse.mm(A, E_c.unsqueeze(-1)).squeeze(-1)
        r = b - Ax

        # è®¡ç®—æ®‹å·®çš„L2èŒƒæ•°å¹³æ–¹: ||Ax - b||_2^2
        phi_loss = torch.norm(r, p=2).pow(2)
        phi_losses.append(phi_loss)

    # ä½¿ç”¨torch.stackå’Œtorch.sumä¿æŒæ¢¯åº¦è®¡ç®—å›¾
    loss_sum = torch.stack(phi_losses).sum()

    return loss_sum

def compute_asinh_loss(E_real_cur, E_imag_cur, batch_y, num_nodes):
    """
    è®¡ç®—AsinhæŸå¤±ï¼šsqrt(asinh(||pred - true||^2))ï¼ˆä¸éœ€è¦èŠ‚ç‚¹å¹³å‡ï¼‰

    Args:
        E_real_cur: é¢„æµ‹çš„å®éƒ¨ç”µåœº [N]
        E_imag_cur: é¢„æµ‹çš„è™šéƒ¨ç”µåœº [N]
        batch_y: çœŸå®æ ‡ç­¾ [N, 2]
        num_nodes: èŠ‚ç‚¹æ€»æ•°ï¼ˆAsinh lossä¸ä½¿ç”¨ï¼Œç”¨äºæ¥å£ç»Ÿä¸€ï¼‰

    Returns:
        loss_value: AsinhæŸå¤±ï¼ˆä¸è¿›è¡ŒèŠ‚ç‚¹å¹³å‡ï¼‰
    """
    true_real = batch_y[:, 0]
    true_imag = batch_y[:, 1]

    # è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®
    diff_real = E_real_cur - true_real
    diff_imag = E_imag_cur - true_imag

    # åˆ†åˆ«è®¡ç®—å®éƒ¨å’Œè™šéƒ¨çš„å¹³å‡å¹³æ–¹è¯¯å·®
    mse_real = diff_real.pow(2).mean()  # å®éƒ¨å¹³å‡å¹³æ–¹è¯¯å·®
    mse_imag = diff_imag.pow(2).mean()  # è™šéƒ¨å¹³å‡å¹³æ–¹è¯¯å·®

    # åˆ†åˆ«åº”ç”¨asinhå‡½æ•°
    asinh_real = torch.asinh(mse_real)
    asinh_imag = torch.asinh(mse_imag)

    # å°†å®éƒ¨å’Œè™šéƒ¨çš„asinhç»“æœç›¸åŠ ï¼Œç„¶åå¼€æ–¹
    loss_sum = torch.sqrt(asinh_real + asinh_imag)

    return loss_sum

def compute_hybrid_loss(E_real_cur, E_imag_cur, batch_y, k_all, node_batch, B, device, matrix_dtype, epoch, num_nodes):
    """
    è®¡ç®—HybridæŸå¤±ï¼šå§‹ç»ˆé‡‡ç”¨MSE + 0.1*Phiçš„å›ºå®šç»„åˆï¼ˆPhi lossè¿›è¡ŒèŠ‚ç‚¹å¹³å‡ï¼‰

    Args:
        E_real_cur: é¢„æµ‹çš„å®éƒ¨ç”µåœº [N]
        E_imag_cur: é¢„æµ‹çš„è™šéƒ¨ç”µåœº [N]
        batch_y: çœŸå®æ ‡ç­¾ [N, 2]
        k_all: æ ·æœ¬ç´¢å¼• [B]
        node_batch: èŠ‚ç‚¹åˆ°æ‰¹æ¬¡çš„æ˜ å°„ [N]
        B: æ‰¹æ¬¡å¤§å°
        device: è®¡ç®—è®¾å¤‡
        matrix_dtype: çŸ©é˜µæ•°æ®ç±»å‹
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        num_nodes: èŠ‚ç‚¹æ€»æ•°

    Returns:
        tuple: (loss_value, mse_loss_sum, phi_loss_sum)
        - loss_value: HybridæŸå¤±ï¼ˆMSE + 0.1*Phiï¼Œè¿›è¡ŒPhièŠ‚ç‚¹å¹³å‡ï¼‰
        - mse_loss_sum: MSE lossæ€»å’Œï¼ˆæœªå¹³å‡ï¼‰
        - phi_loss_sum: Phi lossæ€»å’Œï¼ˆæœªå¹³å‡ï¼‰
    """
    # è®¡ç®—MSE lossï¼ˆæ€»å’Œï¼Œæœªå¹³å‡ï¼‰
    mse_loss_sum = compute_mse_loss(E_real_cur, E_imag_cur, batch_y, num_nodes)

    # è®¡ç®—Phi lossï¼ˆæ€»å’Œï¼Œæœªå¹³å‡ï¼‰
    phi_loss_sum = compute_phi_loss(E_real_cur, E_imag_cur, k_all, node_batch, B, device, matrix_dtype, num_nodes)

    # Phi lossè¿›è¡ŒèŠ‚ç‚¹å¹³å‡ï¼šæ€»Phi lossé™¤ä»¥èŠ‚ç‚¹æ•°num_nodes
    phi_loss_per_node = phi_loss_sum / num_nodes

    # å›ºå®šæƒé‡ç­–ç•¥ï¼šå§‹ç»ˆä½¿ç”¨MSE + 0.1*Phi
    lambda_phi = 0.5

    # ç»„åˆæŸå¤±ï¼š(MSE_sum / num_nodes) + 0.1 * Phi_per_node
    loss_value = (mse_loss_sum / num_nodes) + lambda_phi * phi_loss_per_node

    return loss_value, mse_loss_sum, phi_loss_sum

# ==========================================
# ==========================================
# 3.5. é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å‡½æ•°
# ==========================================
def load_pretrained_model(model_dir, solver, n_iter, device, is_main_process):
    """
    åªåŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼ˆä¸åŠ è½½ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ç­‰çŠ¶æ€ï¼‰
    ç”¨äºè¿ç§»å­¦ä¹ ï¼šåœ¨æ–°æ•°æ®é›†ä¸Šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç»§ç»­è®­ç»ƒ
    
    Args:
        model_dir: æ¨¡å‹æƒé‡æ–‡ä»¶ç›®å½•
        solver: æ¨¡å‹
        n_iter: è¿­ä»£æ¬¡æ•°
        device: è®¾å¤‡
        is_main_process: æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
    """
    if is_main_process:
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡: {model_dir}")
    
    # è·å–åŸå§‹æ¨¡å‹ï¼ˆå»é™¤DDPåŒ…è£…ï¼‰
    raw_model = solver.module if isinstance(solver, (DataParallel, DDP)) else solver
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    all_exist = True
    for i in range(n_iter):
        real_path = os.path.join(model_dir, f"real_iter_{i}.pth")
        imag_path = os.path.join(model_dir, f"imag_iter_{i}.pth")
        if not os.path.exists(real_path) or not os.path.exists(imag_path):
            all_exist = False
            if is_main_process:
                print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {real_path} æˆ– {imag_path}")
            break
    
    if not all_exist:
        if is_main_process:
            print("   å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
        return False
    
    # åŠ è½½æ¨¡å‹æƒé‡
    try:
        for i in range(n_iter):
            real_path = os.path.join(model_dir, f"real_iter_{i}.pth")
            imag_path = os.path.join(model_dir, f"imag_iter_{i}.pth")
            
            real_net = raw_model.model_real.get_network(i)
            imag_net = raw_model.model_imag.get_network(i)
            
            real_net.load_state_dict(torch.load(real_path, map_location=device, weights_only=True))
            imag_net.load_state_dict(torch.load(imag_path, map_location=device, weights_only=True))
        
        if is_main_process:
            print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
            print(f"   å·²åŠ è½½ {n_iter} ä¸ªè¿­ä»£ç½‘ç»œçš„æƒé‡")
            print(f"   âš ï¸  æ³¨æ„ï¼šä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰çŠ¶æ€å·²é‡æ–°åˆå§‹åŒ–")
        
        return True
    except Exception as e:
        if is_main_process:
            print(f"âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            print("   å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
        return False


# 4. ç‰©ç†æ±‚è§£å™¨å°è£… (æ ¸å¿ƒé€»è¾‘)
# ==========================================
class PhiSAGESolver(nn.Module):
    """
    å°†ç‰©ç†è¿­ä»£å¾ªç¯å°è£…ä¸º Moduleï¼Œä»¥ä¾¿ DataParallel å¯ä»¥è‡ªåŠ¨åˆ†å‘è®¡ç®—ã€‚
    """
    def __init__(self, input_feats, output_feats, n_iter=N_ITER):
        super(PhiSAGESolver, self).__init__()
        self.n_iter = n_iter
        # å†…éƒ¨å®ä¾‹åŒ–ä¸¤ä¸ªæ¨¡å‹
        self.model_real = BuildGCNList(input_feats, output_feats, n_iter)
        self.model_imag = BuildGCNList(input_feats, output_feats, n_iter)
    
    def forward(self, data_list, epoch=None):
        """
        DP æ¨¡å¼ä¸‹ï¼Œdata_list æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆåŸæœ¬ Batch çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
        æˆ‘ä»¬éœ€è¦åœ¨å½“å‰ GPU ä¸Šå°†å…¶ Collate æˆä¸€ä¸ª Batchï¼Œç„¶åè·‘ç‰©ç†å¾ªç¯ã€‚

        Args:
            data_list: æ•°æ®åˆ—è¡¨
            epoch: å½“å‰è®­ç»ƒè½®æ¬¡ï¼Œç”¨äºhybrid lossè®¡ç®—ï¼ˆå¯é€‰ï¼‰
        """
        # 1. ç¡®ä¿ data_list ä¸­çš„å…ƒç´ éƒ½æ˜¯ Data å¯¹è±¡
        # DataParallel å¯èƒ½ä¼šä¼ é€’ç‰¹æ®Šæ ¼å¼çš„æ•°æ®ï¼Œéœ€è¦å¤„ç†
        if not isinstance(data_list, list):
            data_list = [data_list]
        
        # æ£€æŸ¥å¹¶è½¬æ¢æ•°æ®æ ¼å¼
        processed_data_list = []
        for item in data_list:
            if isinstance(item, Data):
                # ç¡®ä¿ Data å¯¹è±¡æœ‰å¿…è¦çš„å±æ€§ï¼Œå¹¶ä¸” k_idx æ˜¯ tensor
                if not hasattr(item, 'k_idx'):
                    # å¦‚æœæ²¡æœ‰ k_idxï¼Œå°è¯•ä»å…¶ä»–å±æ€§è·å–æˆ–è®¾ç½®é»˜è®¤å€¼
                    item.k_idx = torch.tensor([0])
                elif not isinstance(item.k_idx, torch.Tensor):
                    item.k_idx = torch.tensor([item.k_idx] if not isinstance(item.k_idx, (list, tuple)) else item.k_idx)
                processed_data_list.append(item)
            elif isinstance(item, tuple):
                # å¦‚æœæ˜¯ tupleï¼Œå¯èƒ½æ˜¯ (data, ...) æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if len(item) > 0 and isinstance(item[0], Data):
                    processed_data_list.append(item[0])
                else:
                    raise TypeError(f"æ— æ³•å¤„ç†çš„æ•°æ®æ ¼å¼: {type(item)}, å†…å®¹: {item}")
            else:
                # å°è¯•ç›´æ¥ä½¿ç”¨ï¼Œå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸
                processed_data_list.append(item)
        
        # 2. åœ¨å½“å‰ GPU ä¸Šæ„å»º Batch
        # æ³¨æ„ï¼šå¦‚æœé‡åˆ° tupleBatch é”™è¯¯ï¼Œå¯èƒ½æ˜¯ PyG ç‰ˆæœ¬é—®é¢˜
        # å°è¯•ä½¿ç”¨ collate å‡½æ•°ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        try:
            batch = Batch.from_data_list(processed_data_list)
        except (AttributeError, TypeError) as e:
            error_msg = str(e)
            if 'stores_as' in error_msg or 'tupleBatch' in error_msg:
                # ä½¿ç”¨ collate å‡½æ•°æ‰‹åŠ¨æ„å»º Batch
                from torch_geometric.data.collate import collate
                try:
                    batch, slice_dict, inc_dict = collate(
                        Data,
                        processed_data_list,
                        increment=True,
                        add_batch=True,
                        follow_batch=[],
                    )
                except Exception as e2:
                    # å¦‚æœ collate ä¹Ÿå¤±è´¥ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    if is_main_process():
                        print(f"âŒ Batch.from_data_list å¤±è´¥: {e}")
                        print(f"âŒ collate ä¹Ÿå¤±è´¥: {e2}")
                        print(f"   æ•°æ®åˆ—è¡¨é•¿åº¦: {len(processed_data_list)}")
                        print(f"   ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(processed_data_list[0]) if processed_data_list else 'None'}")
                        if processed_data_list:
                            print(f"   ç¬¬ä¸€ä¸ªå…ƒç´ çš„å±æ€§: {dir(processed_data_list[0])}")
                    raise e
            else:
                raise e
        
        # ç¡®ä¿ batch åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        # åœ¨å¤šå¡æƒ…å†µä¸‹ï¼ŒDataParallel ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…ï¼Œbatch å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        # åœ¨å•å¡æƒ…å†µä¸‹ï¼Œéœ€è¦ç¡®ä¿ batch åœ¨æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Š
        device = batch.x.device
        
        # å®‰å…¨åœ°è·å–æ¨¡å‹è®¾å¤‡ï¼ˆé¿å…åœ¨ DataParallel replica ä¸­å‡ºé”™ï¼‰
        try:
            # å°è¯•è·å–æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡
            model_device = next(self.parameters()).device
            # å¦‚æœ batch ä¸åœ¨æ¨¡å‹è®¾å¤‡ä¸Šï¼Œåˆ™ç§»åŠ¨ batchï¼ˆä¸»è¦ç”¨äºå•å¡æƒ…å†µï¼‰
            if device != model_device:
                batch = batch.to(model_device)
                device = model_device
        except (StopIteration, RuntimeError):
            # åœ¨ DataParallel çš„ replica ä¸­ï¼Œå‚æ•°å¯èƒ½ä¸å¯ç”¨
            # æ­¤æ—¶ batch å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆç”± DataParallel ä¿è¯ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ batch çš„è®¾å¤‡
            pass
        
        # 2. å‡†å¤‡æ•°æ®
        k_all = batch.k_idx
        node_batch = batch.batch
        B = k_all.size(0)
        
        # ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ cloneï¼Œä½¿ç”¨ view æˆ–ç›´æ¥ç´¢å¼•
        # èŠ‚ç‚¹ç‰¹å¾ï¼š[eps_re, eps_im, r_re, r_im, Ebz_re, Ebz_im, bg_re, bg_im]
        # æ ¹æ®æ¨¡å‹ç²¾åº¦å†³å®šæ•°æ®ç±»å‹ - å…¼å®¹ä¸åŒå±‚ç±»å‹
        if hasattr(self.model_real.networks[0].gcn1, 'conv'):
            # GCNå±‚çš„æƒ…å†µ
            is_double = self.model_real.networks[0].gcn1.conv.lin_fusion.weight.dtype == torch.float64
        elif hasattr(self.model_real.networks[0].gcn1, 'spatial_conv'):
            # SpectralGCNå±‚çš„æƒ…å†µ
            is_double = self.model_real.networks[0].gcn1.spatial_conv.lin_fusion.weight.dtype == torch.float64
        elif hasattr(self.model_real.networks[0].gcn1, 'linear'):
            # FFTLayerçš„æƒ…å†µ
            is_double = self.model_real.networks[0].gcn1.linear.weight.dtype == torch.float64
        else:
            # é»˜è®¤æƒ…å†µ
            is_double = False

        if is_double:
            eps_feat = batch.x[:, 0:2].double()  # L-BFGSé˜¶æ®µç”¨double
            bg_real = batch.x[:, 6].double()     # èƒŒæ™¯åœºå®éƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
            bg_imag = batch.x[:, 7].double()     # èƒŒæ™¯åœºè™šéƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
            current_E_real = batch.x[:, 4].double()  # åˆå§‹ç”µåœºå®éƒ¨
            current_E_imag = batch.x[:, 5].double()  # åˆå§‹ç”µåœºè™šéƒ¨
        else:
            eps_feat = batch.x[:, 0:2].float()   # Adamé˜¶æ®µç”¨float
            bg_real = batch.x[:, 6].float()      # èƒŒæ™¯åœºå®éƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
            bg_imag = batch.x[:, 7].float()      # èƒŒæ™¯åœºè™šéƒ¨ (ä¸éšç½‘ç»œæ›´æ–°)
            current_E_real = batch.x[:, 4].float()  # åˆå§‹ç”µåœºå®éƒ¨
            current_E_imag = batch.x[:, 5].float()  # åˆå§‹ç”µåœºè™šéƒ¨
        
        # 3. ä¼˜åŒ–ï¼šåœ¨ç‰©ç†è¿­ä»£å¾ªç¯ä¹‹å‰ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰çŸ©é˜µåˆ° GPU
        # è¿™æ ·å¯ä»¥åœ¨å¤šæ¬¡è¿­ä»£ä¸­é‡å¤ä½¿ç”¨ï¼Œé¿å…é‡å¤çš„ CPU->GPU ä¼ è¾“ï¼ˆæ€§èƒ½å…³é”®ä¼˜åŒ–ï¼‰
        A_list = []
        b_list = []
        # æ ¹æ®æ¨¡å‹ç²¾åº¦å†³å®šçŸ©é˜µç²¾åº¦ - å…¼å®¹ä¸åŒå±‚ç±»å‹
        if hasattr(self.model_real.networks[0].gcn1, 'conv'):
            model_dtype = self.model_real.networks[0].gcn1.conv.lin_fusion.weight.dtype
        elif hasattr(self.model_real.networks[0].gcn1, 'spatial_conv'):
            model_dtype = self.model_real.networks[0].gcn1.spatial_conv.lin_fusion.weight.dtype
        elif hasattr(self.model_real.networks[0].gcn1, 'linear'):
            model_dtype = self.model_real.networks[0].gcn1.linear.weight.dtype
        else:
            model_dtype = torch.float32

        matrix_dtype = torch.complex128 if model_dtype == torch.float64 else torch.complex64

        for b_idx in range(B):
            k = int(k_all[b_idx].item())
            A, b = get_Ab(k, device, matrix_dtype)
            A_list.append(A)
            b_list.append(b)
        
        # åªä¿å­˜å½“å‰è¿­ä»£çš„ç»“æœï¼Œè€Œä¸æ˜¯æ‰€æœ‰å†å²
        E_real_cur = current_E_real
        E_imag_cur = current_E_imag
        
        # 4. ç‰©ç†è¿­ä»£å¾ªç¯
        for iter_idx in range(self.n_iter):
            r_real_list = []
            r_imag_list = []
            
            # è®¡ç®—æ®‹å·® r = b - A*E
            # ä¼˜åŒ–ï¼šä½¿ç”¨å·²åŠ è½½çš„çŸ©é˜µï¼ˆå·²åœ¨ GPU ä¸Šï¼‰ï¼Œé¿å…é‡å¤ä¼ è¾“
            with torch.no_grad():
                for b_idx in range(B):
                    mask = (node_batch == b_idx)
                    E_r = E_real_cur[mask]
                    E_i = E_imag_cur[mask]
                    
                    # ä½¿ç”¨å·²åŠ è½½çš„çŸ©é˜µï¼ˆå·²åœ¨ GPU ä¸Šï¼Œæ— éœ€é‡å¤ä¼ è¾“ï¼‰
                    A = A_list[b_idx]
                    b_vec = b_list[b_idx]
                    
                    E_c = torch.complex(E_r, E_i)
                    Ax = torch.sparse.mm(A, E_c.unsqueeze(-1)).squeeze(-1)
                    r_c = b_vec - Ax
                    
                    # æ ¹æ®å½“å‰æ¨¡å‹ç²¾åº¦å†³å®šæ•°æ®ç±»å‹ - å…¼å®¹ä¸åŒå±‚ç±»å‹
                    if hasattr(self.model_real.networks[0].gcn1, 'conv'):
                        use_double = self.model_real.networks[0].gcn1.conv.lin_fusion.weight.dtype == torch.float64
                    elif hasattr(self.model_real.networks[0].gcn1, 'spatial_conv'):
                        use_double = self.model_real.networks[0].gcn1.spatial_conv.lin_fusion.weight.dtype == torch.float64
                    elif hasattr(self.model_real.networks[0].gcn1, 'linear'):
                        use_double = self.model_real.networks[0].gcn1.linear.weight.dtype == torch.float64
                    else:
                        use_double = False

                    if use_double:
                        r_real_list.append(r_c.real.double())
                        r_imag_list.append(r_c.imag.double())
                    else:
                        r_real_list.append(r_c.real.float())
                        r_imag_list.append(r_c.imag.float())
                    
                    # é‡Šæ”¾ä¸­é—´å˜é‡ï¼ˆä½†ä¿ç•™çŸ©é˜µ A å’Œ bï¼Œå› ä¸ºè¿˜è¦åœ¨ä¸‹æ¬¡è¿­ä»£ä¸­ä½¿ç”¨ï¼‰
                    del E_c, Ax, r_c
            
            r_real = torch.cat(r_real_list, dim=0)
            r_imag = torch.cat(r_imag_list, dim=0)
            
            # ä¼˜åŒ–ï¼šé‡Šæ”¾ä¸­é—´åˆ—è¡¨
            del r_real_list, r_imag_list
            
            # æ„é€ è¾“å…¥ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨ view è€Œä¸æ˜¯ unsqueezeï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
            # èŠ‚ç‚¹ç‰¹å¾åŒ…å«ï¼š[eps, r, E_current, bg] å…±8ä¸ªé€šé“
            x_in = torch.cat([
                eps_feat,                # eps_real, eps_imag [N, 2]
                r_real.view(-1, 1),      # r_real [N, 1]
                r_imag.view(-1, 1),      # r_imag [N, 1]
                E_real_cur.view(-1, 1),  # å½“å‰ç”µåœºå®éƒ¨ [N, 1] (éšè¿­ä»£æ›´æ–°)
                E_imag_cur.view(-1, 1),  # å½“å‰ç”µåœºè™šéƒ¨ [N, 1] (éšè¿­ä»£æ›´æ–°)
                bg_real.view(-1, 1),     # èƒŒæ™¯åœºå®éƒ¨ [N, 1] (ä¸éšç½‘ç»œæ›´æ–°)
                bg_imag.view(-1, 1)      # èƒŒæ™¯åœºè™šéƒ¨ [N, 1] (ä¸éšç½‘ç»œæ›´æ–°)
            ], dim=1)
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜ï¼ˆåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
            use_checkpoint = USE_GRADIENT_CHECKPOINTING

            # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if self.training and use_checkpoint:
                # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šåœ¨å‰å‘ä¼ æ’­æ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
                # è¿™ä¼šèŠ‚çœæ˜¾å­˜ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æ—¶é—´ï¼ˆçº¦20-30%ï¼‰
                def gcn_forward_real(x, edge_index, batch, iter_idx):
                    return self.model_real(x, edge_index, batch, iter_idx)
                def gcn_forward_imag(x, edge_index, batch, iter_idx):
                    return self.model_imag(x, edge_index, batch, iter_idx)

                delta_real = checkpoint(gcn_forward_real, x_in, batch.edge_index, batch.batch, iter_idx, use_reentrant=False)
                delta_imag = checkpoint(gcn_forward_imag, x_in, batch.edge_index, batch.batch, iter_idx, use_reentrant=False)
            else:
                # æ­£å¸¸å‰å‘ä¼ æ’­
                delta_real = self.model_real(x_in, batch.edge_index, batch.batch, iter_idx)
                delta_imag = self.model_imag(x_in, batch.edge_index, batch.batch, iter_idx)
            
            # å±•å¹³å¹¶æ›´æ–°ï¼ˆä½¿ç”¨ in-place æ“ä½œèŠ‚çœæ˜¾å­˜ï¼‰
            delta_real = delta_real.view(-1)
            delta_imag = delta_imag.view(-1)
            
            # ä¼˜åŒ–ï¼šç›´æ¥æ›´æ–°ï¼Œä¸ä¿å­˜å†å²ï¼ˆåªä¿ç•™å½“å‰å€¼ï¼‰
            E_real_next = E_real_cur + delta_real
            E_imag_next = E_imag_cur + delta_imag
            
            # ä¼˜åŒ–ï¼šé‡Šæ”¾ä¸­é—´å˜é‡
            del x_in, delta_real, delta_imag, r_real, r_imag
            
            # æ›´æ–°å½“å‰å€¼ï¼ˆä¸ºä¸‹ä¸€æ¬¡è¿­ä»£å‡†å¤‡ï¼‰
            E_real_cur = E_real_next
            E_imag_cur = E_imag_next
            
            # ä¼˜åŒ–ï¼šä¸åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ¸…ç†æ˜¾å­˜ï¼Œé¿å…é˜»å¡
            # ä»…åœ¨é˜¶æ®µåˆ‡æ¢æ—¶æ¸…ç†æ˜¾å­˜ç¢ç‰‡
        
        # 5. è®¡ç®—æŸå¤±
        num_nodes = batch.x.size(0)  # æ€»èŠ‚ç‚¹æ•°
        if LOSS_TYPE == "hybrid":
            # compute_hybrid_lossç°åœ¨è¿”å›ä¸‰ä¸ªå€¼ï¼Œé¿å…é‡å¤è®¡ç®—
            loss_sum, mse_loss_sum, phi_loss_sum = compute_hybrid_loss(E_real_cur, E_imag_cur, batch.y, k_all, node_batch, B, device, matrix_dtype, epoch, num_nodes)
        else:
            # éhybridæ¨¡å¼ä¸‹ï¼Œåªè®¡ç®—å®é™…éœ€è¦çš„loss
            if LOSS_TYPE == "phi":
                loss_sum = compute_phi_loss(E_real_cur, E_imag_cur, k_all, node_batch, B, device, matrix_dtype, num_nodes)
            elif LOSS_TYPE == "asinh":
                loss_sum = compute_asinh_loss(E_real_cur, E_imag_cur, batch.y, num_nodes)
            else:
                # é»˜è®¤ä½¿ç”¨MSE loss
                loss_sum = compute_mse_loss(E_real_cur, E_imag_cur, batch.y, num_nodes)

            # éhybridæ¨¡å¼ä¸‹ä¸éœ€è¦å•ç‹¬çš„MSEå’ŒPhi lossï¼Œä½¿ç”¨å ä½ç¬¦
            mse_loss_sum = torch.tensor(0.0, device=device)
            phi_loss_sum = torch.tensor(0.0, device=device)

        # 6. è®¡ç®—ç›¸å¯¹è¯¯å·®å½¢å¼çš„RES loss
        # RES losséœ€è¦çœŸå®æ ‡ç­¾ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹è¯¯å·®
        true_real = batch.y[:, 0]
        true_imag = batch.y[:, 1]
        # RES loss = ||x^FEM - x^GCN||_2^2 / ||x^FEM||_2^2
        # å…¶ä¸­ x^FEM æ˜¯çœŸå®è§£ï¼Œx^GCN æ˜¯é¢„æµ‹è§£
        # ||x||_2^2 = sum_i |x_i|^2ï¼Œè®¡ç®—å¹³æ–¹çš„ç›¸å¯¹è¯¯å·®ï¼ˆä¸å…¬å¼ä¸€è‡´ï¼‰
        res_loss_sum = torch.tensor(0.0, device=device, dtype=loss_sum.dtype)
        with torch.no_grad():
            for b_idx in range(B):
                mask = (node_batch == b_idx)
                # é¢„æµ‹è§£ x^GCN
                pred_real = E_real_cur[mask]
                pred_imag = E_imag_cur[mask]
                # çœŸå®è§£ x^FEM
                true_r = true_real[mask]
                true_i = true_imag[mask]
                
                # è®¡ç®— ||x^FEM - x^GCN||_2^2 = sum_i |true_i - pred_i|^2
                # å¯¹äºå¤æ•°å‘é‡ï¼Œéœ€è¦åˆ†åˆ«è®¡ç®—å®éƒ¨å’Œè™šéƒ¨
                diff_real = true_r - pred_real
                diff_imag = true_i - pred_imag
                # æå‰å–ç»å¯¹å€¼å†å¹³æ–¹
                numerator = (torch.abs(diff_real).pow(2).sum() + torch.abs(diff_imag).pow(2).sum())
                
                # è®¡ç®— ||x^FEM||_2^2 = sum_i |true_i|^2
                # æå‰å–ç»å¯¹å€¼å†å¹³æ–¹
                denominator = (torch.abs(true_r).pow(2).sum() + torch.abs(true_i).pow(2).sum())
                
                # é¿å…é™¤é›¶ï¼Œå¦‚æœåˆ†æ¯å¤ªå°åˆ™ä½¿ç”¨ä¸€ä¸ªå°çš„epsilon
                epsilon = 1e-10
                if denominator > epsilon:
                    # å¹³æ–¹çš„ç›¸å¯¹è¯¯å·®ï¼š||x^FEM - x^GCN||_2^2 / ||x^FEM||_2^2ï¼ˆä¸å…¬å¼ä¸€è‡´ï¼‰
                    rel_error_squared = numerator / denominator
                    res_loss_sum = res_loss_sum + rel_error_squared
                else:
                    # å¦‚æœçœŸå®è§£èŒƒæ•°å¤ªå°ï¼Œä½¿ç”¨ç»å¯¹è¯¯å·®çš„å¹³æ–¹
                    res_loss_sum = res_loss_sum + numerator
                
                # é‡Šæ”¾ä¸­é—´å˜é‡
                del diff_real, diff_imag, pred_real, pred_imag, true_r, true_i
        
        # ä¼˜åŒ–ï¼šåœ¨è¿­ä»£ç»“æŸåé‡Šæ”¾çŸ©é˜µåˆ—è¡¨ï¼ˆé‡Šæ”¾æ˜¾å­˜ï¼‰
        del A_list, b_list
        
        # é‡Šæ”¾å˜é‡
        del E_real_cur, E_imag_cur, true_real, true_imag
        
        # æ³¨æ„ï¼šPyG DataParallel å¯èƒ½ä¸æ”¯æŒ tuple è¿”å›å€¼ï¼Œä¼šå°è¯•å°†å…¶å½“ä½œ Batch å¤„ç†
        # å› æ­¤è¿”å›å•ä¸ª tensorï¼š[loss_sum, res_loss_sum, num_nodes, num_samples, mse_loss_sum, phi_loss_sum]
        # è¿™æ ·å¯ä»¥é¿å… 'tupleBatch' é”™è¯¯
        # num_nodes: æ€»èŠ‚ç‚¹æ•°ï¼ˆç”¨äºlossçš„å¹³å‡ï¼‰
        # num_samples: æ€»æ ·æœ¬æ•°ï¼ˆç”¨äºRES lossçš„å¹³å‡ï¼Œå› ä¸ºç›¸å¯¹è¯¯å·®æ˜¯é’ˆå¯¹æ¯ä¸ªæ ·æœ¬çš„ï¼‰
        num_nodes = batch.x.size(0)  # æ€»èŠ‚ç‚¹æ•°ï¼ˆä» batch.x è·å–ï¼Œä¸éœ€è¦ä» E_real_curï¼‰
        num_samples = B  # æ ·æœ¬æ•°ï¼ˆbatchä¸­çš„å›¾æ•°é‡ï¼‰
        num_nodes_tensor = torch.tensor(num_nodes, dtype=torch.float32, device=device)
        num_samples_tensor = torch.tensor(num_samples, dtype=torch.float32, device=device)
        mse_loss_tensor = mse_loss_sum.to(dtype=torch.float32, device=device) if hasattr(mse_loss_sum, 'to') else torch.tensor(float(mse_loss_sum), dtype=torch.float32, device=device)
        phi_loss_tensor = phi_loss_sum.to(dtype=torch.float32, device=device) if hasattr(phi_loss_sum, 'to') else torch.tensor(float(phi_loss_sum), dtype=torch.float32, device=device)
        # è¿”å›å½¢çŠ¶ä¸º [6] çš„ tensor: [loss_sum, res_loss_sum, num_nodes, num_samples, mse_loss_sum, phi_loss_sum]
        return torch.stack([loss_sum, res_loss_sum, num_nodes_tensor, num_samples_tensor, mse_loss_tensor, phi_loss_tensor])



# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def setup_ddp(rank, world_size):
    """è®¾ç½®DDPç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = MASTER_PORT

    # è®¾ç½®NCCLä¼˜åŒ–ç¯å¢ƒå˜é‡ï¼Œæé«˜åˆ†å¸ƒå¼è®­ç»ƒç¨³å®šæ€§
    os.environ['NCCL_TIMEOUT'] = '1800000'     # 30åˆ†é’Ÿè¶…æ—¶ (æ¯«ç§’)
    os.environ['NCCL_IB_DISABLE'] = '1'         # ç¦ç”¨IBä»¥æé«˜å…¼å®¹æ€§
    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'     # ä½¿ç”¨æœ¬åœ°ç¯å›æ¥å£
    os.environ['NCCL_DEBUG'] = 'WARN'           # è®¾ç½®è°ƒè¯•çº§åˆ«

    # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œè®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆ30åˆ†é’Ÿï¼‰ä»¥é¿å…NCCLè¶…æ—¶
    dist.init_process_group("nccl", rank=rank, world_size=world_size, timeout=timedelta(minutes=30))

    # è®¾ç½®å½“å‰è¿›ç¨‹çš„GPU
    torch.cuda.set_device(rank)
    device = torch.device(f'cuda:{rank}')

    return device

def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    dist.destroy_process_group()

def main_worker(rank, world_size):
    """DDPè®­ç»ƒçš„ä¸»å·¥ä½œå‡½æ•°"""
    device = setup_ddp(rank, world_size)

    # åªæœ‰ä¸»è¿›ç¨‹(rank 0)è¾“å‡ºä¿¡æ¯
    is_main_process = (rank == 0)

    # DDPç¯å¢ƒä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹åªè´Ÿè´£ä¸€ä¸ªGPU
    device_ids = [rank]
    
    if is_main_process:
        print(f"ğŸš€ å¯åŠ¨ DDP åˆ†å¸ƒå¼è®­ç»ƒ")
        print(f"   è¿›ç¨‹ {rank}/{world_size}")
        print(f"   GPU: {device}")
        print(f"   æ¯ä¸ªGPUçš„BatchSize: {TOTAL_BATCH_SIZE}")
        print(f"   æ€»BatchSize: {TOTAL_BATCH_SIZE * world_size}")
        print(f"   NCCLè¶…æ—¶æ—¶é—´: 30åˆ†é’Ÿ")
        print(f"   ç¯å¢ƒå˜é‡: NCCL_TIMEOUT=1800000ms")

        # æ‰“å°æ•°æ®é›†é…ç½®ä¿¡æ¯ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°ï¼‰
        from config import DATASET_TYPE, DATA_ROOT_PATH, SCA_PREFIX
        print(f"ğŸ“Š å…¨å±€æ•°æ®é›†é…ç½®: {DATASET_TYPE}")
        print(f"   æ•°æ®æ ¹ç›®å½•: {DATA_ROOT_PATH}")
        print(f"   æ–‡ä»¶å‘½åå‰ç¼€: {SCA_PREFIX}")
    
    # 2. æ•°æ®å‡†å¤‡
    data_mapping, n_total = scan_all_data(root_data_path)
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ„å»ºæ•°æ®é›†ï¼ˆå¦‚æœæ•°æ®é‡å¤§ï¼‰
    dataset = []

    # è·å–æ‰€æœ‰æœ‰æ•ˆçš„ k ç´¢å¼•
    valid_k_list = [k for k in range(1, n_total + 1) if k in data_mapping]

    # ä½¿ç”¨ä¸²è¡Œæ–¹å¼æ„å»ºæ•°æ®é›†ï¼ˆå¤šè¿›ç¨‹ä¼šå¯¼è‡´å…±äº«å†…å­˜é—®é¢˜ï¼‰
    # æ³¨æ„ï¼šå¤šè¿›ç¨‹ä¼ é€’å¤§é‡ Data å¯¹è±¡æ—¶ï¼ŒPython çš„ multiprocessing ä½¿ç”¨å…±äº«å†…å­˜
    # å¯èƒ½å¯¼è‡´ "Too many open files" é”™è¯¯ï¼Œå› æ­¤ä½¿ç”¨ä¸²è¡Œæ–¹å¼æ›´ç¨³å®š
    if is_main_process:
        print("æ„å»ºæ•°æ®é›†...")
        print("   ä¸²è¡Œæ„å»ºæ•°æ®é›†ï¼ˆç¨³å®šå¯é ï¼‰...")
    for idx, k in enumerate(valid_k_list):
        try:
            data = build_graph_data(k)
            if data is not None:
                dataset.append(data)
            # æ¯ 100 ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 1000 == 0 and is_main_process:
                print(f"   è¿›åº¦: {idx + 1}/{len(valid_k_list)} ({100*(idx+1)/len(valid_k_list):.1f}%)")
        except Exception as e:
            if (idx + 1) % 1000 == 0 and is_main_process:  # åªåœ¨æ˜¾ç¤ºè¿›åº¦æ—¶æ‰“å°é”™è¯¯
                print(f"   æ„å»ºæ ·æœ¬ {k} å¤±è´¥: {e}")
            pass
    
    if len(dataset) == 0:
        raise RuntimeError("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒï¼")

    if is_main_process:
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
    train_ds, test_ds = train_test_split(dataset, test_size=0.2, random_state=42)

    # # å®šä¹‰æµ‹è¯•é›†çš„ç¼–å·ï¼ˆä»1å¼€å§‹è®¡æ•°ï¼Œå¯¹åº”æ•°æ®ç´¢å¼•ï¼‰
    # test_indices = [1,13,14,18,19,24,25,36,37,42,43,48,49,60,61,66,67,72,73,84,85,90,91,96]

    # # æ ¹æ®ç¼–å·åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # train_ds = []
    # test_ds = []

    # for i, data in enumerate(dataset):
    #     # data.k_idx å­˜å‚¨çš„æ˜¯æ•°æ®çš„ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
    #     data_idx = data.k_idx.item()
    #     if data_idx in test_indices:
    #         test_ds.append(data)
    #     else:
    #         train_ds.append(data)

    if is_main_process:
        print(f"   è®­ç»ƒé›†: {len(train_ds)}, æµ‹è¯•é›†: {len(test_ds)}")
    # print(f"   æµ‹è¯•é›†ç¼–å·: {test_indices}")
    
    # ã€å…³é”®ã€‘ä½¿ç”¨ DataListLoader
    # æ³¨æ„ï¼šPyG çš„ DataListLoader å¯èƒ½ä¸æ”¯æŒ num_workersï¼ˆä¸“ä¸º DataParallel è®¾è®¡ï¼‰
    # ä½†æˆ‘ä»¬å¯ä»¥å°è¯•è®¾ç½®ï¼Œå¦‚æœä¸æ”¯æŒä¼šè‡ªåŠ¨å¿½ç•¥
    # ä¼˜åŒ–ï¼šæ·»åŠ  pin_memory ä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU

    # Adamé˜¶æ®µï¼šå¼€å¯shuffleå¢åŠ è®­ç»ƒéšæœºæ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
    train_loader = DataListLoader(
    train_ds,
    batch_size=TOTAL_BATCH_SIZE,
    shuffle=True,  # Adamé˜¶æ®µå¼€å¯shuffle
    drop_last=True,
    pin_memory=True,  # ç¡®ä¿ä¸ºTrue
    num_workers=NUM_WORKERS,  # æ·»åŠ è¿™è¡Œï¼å…³é”®ä¼˜åŒ–
    persistent_workers=True if NUM_WORKERS > 0 else False  # ä¿æŒworkerè¿›ç¨‹
)

    # æµ‹è¯•é˜¶æ®µï¼šå…³é—­shuffleï¼Œç¡®ä¿è¯„ä¼°ç»“æœä¸€è‡´æ€§
    test_loader = DataListLoader(
    test_ds,
    batch_size=TOTAL_BATCH_SIZE,
    shuffle=False,  # æµ‹è¯•é˜¶æ®µå…³é—­shuffle
    pin_memory=True,
    num_workers=NUM_WORKERS,  # æ·»åŠ 
    persistent_workers=True if NUM_WORKERS > 0 else False
)
    
    # 3. é¢„åŠ è½½çŸ©é˜µåˆ°æ‰€æœ‰ GPU
    load_matrix_to_cache(data_mapping, n_total, device_ids, is_main_process)
    
    # ã€å…³é”®ã€‘DDPåŒæ­¥ç‚¹ï¼šç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®ŒæˆçŸ©é˜µé¢„åŠ è½½åå†ç»§ç»­
    # è¿™å¯ä»¥é¿å…è¿›ç¨‹é—´ä¸åŒæ­¥å¯¼è‡´çš„NCCLå¿ƒè·³è¶…æ—¶
    if dist.is_initialized():
        dist.barrier()
        if is_main_process:
            print("âœ… æ‰€æœ‰è¿›ç¨‹å·²å®ŒæˆçŸ©é˜µé¢„åŠ è½½ï¼Œç»§ç»­è®­ç»ƒ...")
    
    # 4. æ¨¡å‹åˆå§‹åŒ–ä¸ DP åŒ…è£…
    # Adamé˜¶æ®µä½¿ç”¨float32åŠ å¿«é€Ÿåº¦ï¼ŒL-BFGSé˜¶æ®µåˆ‡æ¢åˆ°float64æé«˜ç²¾åº¦
    # èŠ‚ç‚¹ç‰¹å¾åŒ…å«ï¼š[eps, r, E_current, bg] å…±8ä¸ªé€šé“
    solver = PhiSAGESolver(input_feats=8, output_feats=1, n_iter=N_ITER).float()  # Adamé˜¶æ®µç”¨float32
    solver.to(device)
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨ torch.compile åŠ é€Ÿï¼ˆå¦‚æœæ”¯æŒï¼‰
    # æ³¨æ„ï¼štorch.compile éœ€è¦åœ¨ DataParallel åŒ…è£…ä¹‹å‰åº”ç”¨
    # ä½¿ç”¨å±€éƒ¨å˜é‡æ¥é¿å…ä¿®æ”¹å…¨å±€å˜é‡
    use_compile = USE_COMPILE
    if use_compile and is_main_process:
        try:
            # æ£€æŸ¥ PyTorch ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ compile
            if hasattr(torch, 'compile'):
                print("âœ… å¯ç”¨ torch.compile ä¼˜åŒ–...")
                print("   âš ï¸  æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´ï¼Œå¯èƒ½è¾ƒæ…¢")
                print("   âš ï¸  å¦‚æœé‡åˆ°é”™è¯¯ï¼Œè¯·å°† USE_COMPILE è®¾ç½®ä¸º False")
                # ä½¿ç”¨ 'reduce-overhead' æ¨¡å¼ï¼Œé€‚åˆå¤šæ¬¡è°ƒç”¨çš„åœºæ™¯
                solver = torch.compile(solver, mode='reduce-overhead')
            else:
                print("âš ï¸  PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ torch.compileï¼Œè·³è¿‡æ­¤ä¼˜åŒ–")
                use_compile = False
        except Exception as e:
            print(f"âš ï¸  torch.compile å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨æœªç¼–è¯‘ç‰ˆæœ¬")
            print(f"   ğŸ’¡ æç¤ºï¼šå¦‚æœé‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·å°† USE_COMPILE è®¾ç½®ä¸º False")
            use_compile = False
    
    # æ‰“å°ç½‘ç»œç»´åº¦ä¿¡æ¯
    if is_main_process:
        from config import NETWORK_USE_CUSTOM_DIMS, NETWORK_BASE_DIM, NETWORK_CUSTOM_DIMS, NETWORK_POOL_RATIOS

        print("\nğŸ” ç½‘ç»œç»“æ„ç»´åº¦ä¿¡æ¯:")
        print(f"   è¿­ä»£æ¬¡æ•° (n_iter): {solver.n_iter}")
        print(f"   è¾“å…¥ç‰¹å¾æ•°: {solver.model_real.input_feats}")
        print(f"   è¾“å‡ºç‰¹å¾æ•°: {solver.model_real.output_feats}")

        # æ˜¾ç¤ºé…ç½®æ¥æº
        print(f"   é…ç½®æ¥æº: config.py")
        if NETWORK_USE_CUSTOM_DIMS:
            print(f"     â€¢ ä½¿ç”¨è‡ªå®šä¹‰ç»´åº¦: {NETWORK_CUSTOM_DIMS}")
        else:
            print(f"     â€¢ ä½¿ç”¨åŸºç¡€ç»´åº¦: {NETWORK_BASE_DIM} (è‡ªåŠ¨è®¡ç®—: [{NETWORK_BASE_DIM}, {NETWORK_BASE_DIM*2}, {NETWORK_BASE_DIM*4}])")
        print(f"     â€¢ æ± åŒ–é…ç½®: {NETWORK_POOL_RATIOS}")

        try:
            # è·å–ç¬¬ä¸€ä¸ªç½‘ç»œæ¥æ˜¾ç¤ºç»´åº¦ä¿¡æ¯
            first_network = solver.model_real.networks[0]

            # æ˜¾ç¤ºç½‘ç»œç»´åº¦é…ç½®
            if hasattr(first_network, 'gcn1') and hasattr(first_network.gcn1, 'conv') and hasattr(first_network.gcn1.conv, 'lin_fusion'):
                gcn1_weight = first_network.gcn1.conv.lin_fusion.weight
                gcn2_weight = first_network.gcn2.conv.lin_fusion.weight
                gcn3_weight = first_network.gcn3.conv.lin_fusion.weight

                print(f"   GCNå±‚å®é™…ç»´åº¦:")
                print(f"     â€¢ gcn1: {gcn1_weight.shape[1]} â†’ {gcn1_weight.shape[0]}")
                print(f"     â€¢ gcn2: {gcn2_weight.shape[1]} â†’ {gcn2_weight.shape[0]}")
                print(f"     â€¢ gcn3: {gcn3_weight.shape[1]} â†’ {gcn3_weight.shape[0]}")
                print(f"   ç½‘ç»œæ¶æ„: U-Neté£æ ¼ (ç¼–ç å™¨-è§£ç å™¨)")

                # è®¡ç®—å‚æ•°é‡
                total_params = sum(p.numel() for p in solver.parameters())
                real_params = sum(p.numel() for p in solver.model_real.parameters())
                imag_params = sum(p.numel() for p in solver.model_imag.parameters())

                print(f"   å‚æ•°ç»Ÿè®¡:")
                print(f"     â€¢ æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
                print(f"     â€¢ Realç½‘ç»œ: {real_params:,} å‚æ•°")
                print(f"     â€¢ Imagç½‘ç»œ: {imag_params:,} å‚æ•°")
                print(f"     â€¢ å•è¿­ä»£ç½‘ç»œ: {real_params // solver.n_iter:,} å‚æ•°")
            else:
                print("   âš ï¸  æ— æ³•è·å–è¯¦ç»†çš„ç½‘ç»œç»´åº¦ä¿¡æ¯")

        except Exception as e:
            print(f"   âš ï¸  è·å–ç½‘ç»œç»´åº¦ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            # ä»ç„¶æ˜¾ç¤ºåŸºæœ¬å‚æ•°é‡ä¿¡æ¯
            total_params = sum(p.numel() for p in solver.parameters())
            print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")

    # DDPåŒ…è£…
    solver = DDP(solver, device_ids=[rank], output_device=rank)
    if is_main_process:
        print("âœ… æ¨¡å‹å·²é€šè¿‡ DDP åŒ…è£…")
    
    # ä½¿ç”¨çº¯float32è®­ç»ƒï¼Œæ— éœ€GradScaler
    
    # 5. ä¼˜åŒ–å™¨
    # æ³¨æ„ï¼šDP åŒ…è£…åï¼Œå‚æ•°åä¼šå¤šå‡º .module å‰ç¼€ï¼Œä½†ä¸å½±å“ optimizer è¯†åˆ«
    # DDPä¼˜åŒ–ï¼šå­¦ä¹ ç‡æŒ‰GPUæ•°é‡çº¿æ€§ç¼©æ”¾ï¼ˆæ€»batch_sizeå¢å¤§ï¼‰
    ddp_lr = LR   # æ¯ä¸ªGPUçš„åŸºç¡€å­¦ä¹ ç‡ä¹˜ä»¥GPUæ•°é‡
    optimizer_adam = optim.Adam(solver.parameters(), lr=ddp_lr)

    # ä½¿ç”¨ReduceLROnPlateauè°ƒåº¦å™¨ï¼ŒåŸºäºéªŒè¯æŸå¤±è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
    scheduler = ReduceLROnPlateau(
        optimizer_adam,
        mode=REDUCE_LR_MODE,
        factor=REDUCE_LR_FACTOR,
        patience=REDUCE_LR_PATIENCE,
        min_lr=REDUCE_LR_MIN_LR
    )
    
    # 6. è®­ç»ƒå¾ªç¯
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€å˜é‡
    best_loss = float('inf')  # ç”¨äºæ—©åœåˆ¤æ–­çš„æœ€ä½³loss
    best_epoch = -1
    best_saved_loss = float('inf')  # ç”¨äºä¿å­˜æ¨¡å‹åˆ¤æ–­çš„æœ€ä½³lossï¼ˆhybridæ¨¡å¼ä¸‹200epochåå¼€å§‹ï¼‰

    # æ—©åœæœºåˆ¶å˜é‡
    early_stopping_counter = 0
    early_stopping_best_loss = float('inf')

    # ç”¨äºè®°å½•losså˜åŒ–æ›²çº¿
    train_losses = []
    test_losses = []
    train_mse_losses = []
    test_mse_losses = []
    train_res_losses = []
    test_res_losses = []

    # è®­ç»ƒæ•°æ®ä¿å­˜
    training_data = {
        'epochs': [],
        'train_losses': [],
        'test_losses': [],
        'train_mse_losses': [],
        'test_mse_losses': [],
        'train_res_losses': [],
        'test_res_losses': [],
        'train_relative_errors': [],
        'test_relative_errors': []
    }

    # Hybrid lossæ¨¡å¼ä¸‹çš„MSEå’ŒPhi lossè®°å½•
    if LOSS_TYPE == "hybrid":
        hybrid_loss_data = {
            'epochs': [],
            'train_mse_losses': [],
            'train_phi_losses': [],
            'test_mse_losses': [],
            'test_phi_losses': []
        }

    # è®°å½•è®­ç»ƒæ€»å¼€å§‹æ—¶é—´å’Œä¸Šæ¬¡æ‰“å°æ—¶é—´
    total_start_time = time.time()
    last_print_time = total_start_time
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if LOAD_PRETRAINED_MODEL:
        pretrained_dir = PRETRAINED_MODEL_DIR if PRETRAINED_MODEL_DIR is not None else SAVE_DIR
        load_success = load_pretrained_model(pretrained_dir, solver, N_ITER, device, is_main_process)
        if load_success and is_main_process:
            print("ğŸ”„ å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œä¼˜åŒ–å™¨ç­‰çŠ¶æ€å·²é‡æ–°åˆå§‹åŒ–")
            print("   å°†ä»epoch 0å¼€å§‹è®­ç»ƒï¼ˆåœ¨æ–°æ•°æ®é›†ä¸Šï¼‰")
    
    for epoch in range(TOTAL_EPOCHS):
        solver.train()
        optimizer = optimizer_adam  # åªä½¿ç”¨Adamä¼˜åŒ–å™¨
        
        epoch_loss_sum = 0.0
        epoch_mse_sum = 0.0
        epoch_res_sum = 0.0
        epoch_phi_sum = 0.0  # Hybrid lossæ¨¡å¼ä¸‹çš„Phi lossç´¯åŠ å™¨
        total_nodes = 0
        total_samples = 0  # ç”¨äºRES lossçš„å¹³å‡ï¼ˆç›¸å¯¹è¯¯å·®æ˜¯é’ˆå¯¹æ¯ä¸ªæ ·æœ¬çš„ï¼‰
        
        # ==========================
        # Adam Training (çº¯float32)
        # ==========================
        for data_list in train_loader:
                optimizer.zero_grad()

                # Forward: list -> (split) -> GPUs -> (run) -> (gather) -> results
                outputs = solver(data_list, epoch)

                # ä½¿ç”¨è¾…åŠ©å‡½æ•°æå– loss å’Œ num_nodes
                batch_loss_sum, batch_res_sum, num_nodes, num_samples, batch_mse_sum, batch_phi_sum = extract_loss_and_num_nodes(outputs)

                # è®¡ç®—å¹³å‡ Lossï¼ˆPhiå’ŒAsinh lossä¸åšèŠ‚ç‚¹å¹³å‡ï¼ŒMSE lossä½¿ç”¨èŠ‚ç‚¹å¹³å‡ï¼‰
                # Hybrid lossæ ¹æ®å½“å‰æƒé‡å†³å®šæ˜¯å¦èŠ‚ç‚¹å¹³å‡
                if num_nodes == 0:
                    continue  # è·³è¿‡ç©ºæ‰¹æ¬¡
                if LOSS_TYPE == "mse":
                    loss_mean = batch_loss_sum / num_nodes
                else:
                    loss_mean = batch_loss_sum

                loss_mean.backward()
                optimizer.step()
                
                epoch_loss_sum += batch_loss_sum.item()
                epoch_res_sum += batch_res_sum.item()
                # åªåœ¨hybridæ¨¡å¼ä¸‹ç´¯åŠ é¢å¤–çš„lossåˆ†é‡
                if LOSS_TYPE == "hybrid":
                    epoch_mse_sum += batch_mse_sum.item()
                    epoch_phi_sum += batch_phi_sum.item()
                total_nodes += num_nodes
                total_samples += num_samples

        # ä¼˜åŒ–ï¼šæ¯10ä¸ªepochæ¸…ç†ä¸€æ¬¡æ˜¾å­˜
        if epoch % 10 == 0:
            torch.cuda.empty_cache()
            

        # ==========================
        # æ—¥å¿—ä¸æµ‹è¯•
        # ==========================
        num_batches = len(train_loader)
        if num_batches > 0:
            # è®¡ç®—å¹³å‡loss
            if LOSS_TYPE == "mse":
                # MSE loss: é™¤ä»¥æ€»èŠ‚ç‚¹æ•°ï¼Œå¾—åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„å¹³å‡loss
                avg_train_loss = epoch_loss_sum / total_nodes if total_nodes > 0 else 0.0
                avg_train_mse = avg_train_loss  # MSEæ¨¡å¼ä¸‹MSE losså°±æ˜¯æ€»loss
            elif LOSS_TYPE == "hybrid":
                # Hybrid loss: é™¤ä»¥batchæ•°ï¼Œå¾—åˆ°æ¯ä¸ªbatchçš„å¹³å‡loss
                avg_train_loss = epoch_loss_sum / num_batches
                avg_train_mse = epoch_mse_sum / total_nodes if total_nodes > 0 else 0.0
            else:
                # Phi/Asinh loss: é™¤ä»¥batchæ•°ï¼Œå¾—åˆ°æ¯ä¸ªbatchçš„å¹³å‡loss
                avg_train_loss = epoch_loss_sum / num_batches
                avg_train_mse = 0.0  # éhybridæ¨¡å¼ä¸‹ä¸è®¡ç®—MSE loss
        else:
            avg_train_loss = 0.0
            avg_train_mse = 0.0

        # RES Loss æ˜¯ Sum of Relative Errorsï¼Œæ‰€ä»¥é™¤ä»¥ total_samples (æ€»å›¾æ•°) æ˜¯å¯¹çš„
        avg_train_res = epoch_res_sum / total_samples if total_samples > 0 else 0.0

        # è®¡ç®—å¹³å‡Phi lossï¼ˆåœ¨hybridæ¨¡å¼ä¸‹ï¼‰
        if LOSS_TYPE == "hybrid":
            avg_train_phi = epoch_phi_sum / total_nodes if total_nodes > 0 else 0.0
        else:
            avg_train_phi = 0.0
        
        # ==========================
        # æµ‹è¯•é›†è¯„ä¼° (åŒç†ä¿®æ­£)
        # ==========================
        solver.eval()
        test_loss_sum = 0.0
        test_mse_sum = 0.0
        test_res_sum = 0.0
        test_phi_sum = 0.0  # Hybrid lossæ¨¡å¼ä¸‹çš„Phi lossç´¯åŠ å™¨
        test_total_nodes = 0
        test_total_samples = 0
        test_num_batches = len(test_loader)

        with torch.no_grad():
            # è¯„ä¼°é˜¶æ®µï¼ˆçº¯float32ï¼‰
            for data_list in test_loader:
                outputs = solver(data_list, epoch)
                batch_loss_sum, batch_res_sum, num_nodes, num_samples, batch_mse_sum, batch_phi_sum = extract_loss_and_num_nodes(outputs)
                
                if num_nodes == 0:
                    continue
                
                # ç´¯åŠ åŸºæœ¬loss
                test_res_sum += batch_res_sum.item()  # ç´¯åŠ RES loss
                test_loss_sum += batch_loss_sum.item() * num_nodes  # è½¬æ¢ä¸ºæ€»æŸå¤±å†ç´¯åŠ 

                # åªåœ¨hybridæ¨¡å¼ä¸‹ç´¯åŠ é¢å¤–çš„lossåˆ†é‡
                if LOSS_TYPE == "hybrid":
                    test_mse_sum += batch_mse_sum.item()  # å·²ç»æ˜¯æ€»æŸå¤±
                    test_phi_sum += batch_phi_sum.item()  # å·²ç»æ˜¯æ€»æŸå¤±
                test_total_nodes += num_nodes
                test_total_samples += num_samples
                test_num_batches += 1
        
        if test_num_batches > 0:
            # æŒ‰æ€»èŠ‚ç‚¹æ•°å¹³å‡loss
            if LOSS_TYPE == "mse":
                # MSE loss: é™¤ä»¥æ€»èŠ‚ç‚¹æ•°ï¼Œå¾—åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„å¹³å‡loss
                avg_test_loss = test_loss_sum / test_total_nodes if test_total_nodes > 0 else 0.0
                avg_test_mse = avg_test_loss  # MSEæ¨¡å¼ä¸‹MSE losså°±æ˜¯æ€»loss
            elif LOSS_TYPE == "hybrid":
                # Hybrid loss: é™¤ä»¥æ€»èŠ‚ç‚¹æ•°
                avg_test_loss = test_loss_sum / test_total_nodes if test_total_nodes > 0 else 0.0
                avg_test_mse = test_mse_sum / test_total_nodes if test_total_nodes > 0 else 0.0
            else:
                # Phi/Asinh loss: é™¤ä»¥æ€»èŠ‚ç‚¹æ•°
                avg_test_loss = test_loss_sum / test_total_nodes if test_total_nodes > 0 else 0.0
                avg_test_mse = 0.0  # éhybridæ¨¡å¼ä¸‹ä¸è®¡ç®—MSE loss
        else:
            avg_test_loss = 0.0
            avg_test_mse = 0.0
            
        avg_test_res = test_res_sum / test_total_samples if test_total_samples > 0 else 0.0

        # è®¡ç®—å¹³å‡Phi lossï¼ˆåœ¨hybridæ¨¡å¼ä¸‹ï¼‰
        if LOSS_TYPE == "hybrid":
            avg_test_phi = test_phi_sum / test_total_nodes if test_total_nodes > 0 else 0.0
        else:
            avg_test_phi = 0.0

        # ä½¿ç”¨ReduceLROnPlateauè°ƒåº¦å™¨
        scheduler.step(avg_test_loss)
        
        # Hybridæ¨¡å¼ä¸‹ä»ä¸€å¼€å§‹å°±è®°å½•æœ€ä½³losså¹¶ä¿å­˜æ¨¡å‹
        if LOSS_TYPE == "hybrid":
            if avg_test_loss < best_saved_loss:
                best_saved_loss = avg_test_loss
                best_epoch = epoch

                # ä¿å­˜æ¨¡å‹
                raw_model = solver.module if isinstance(solver, (DataParallel, DDP)) else solver
                os.makedirs(SAVE_DIR, exist_ok=True)

                # ä¿å­˜æ¨¡å‹æƒé‡
                n_iter = raw_model.n_iter
                for i in range(n_iter):
                    torch.save(raw_model.model_real.get_network(i).state_dict(),
                               os.path.join(SAVE_DIR, f"real_iter_{i}.pth"))
                    torch.save(raw_model.model_imag.get_network(i).state_dict(),
                               os.path.join(SAVE_DIR, f"imag_iter_{i}.pth"))
        else:
            # éhybridæ¨¡å¼ï¼Œæ­£å¸¸ä¿å­˜é€»è¾‘
            if avg_test_loss < best_saved_loss:
                best_saved_loss = avg_test_loss
                best_epoch = epoch

                # ä¿å­˜æ¨¡å‹
                raw_model = solver.module if isinstance(solver, (DataParallel, DDP)) else solver
                os.makedirs(SAVE_DIR, exist_ok=True)

                # ä¿å­˜æ¨¡å‹æƒé‡
                n_iter = raw_model.n_iter
                for i in range(n_iter):
                    torch.save(raw_model.model_real.get_network(i).state_dict(),
                               os.path.join(SAVE_DIR, f"real_iter_{i}.pth"))
                    torch.save(raw_model.model_imag.get_network(i).state_dict(),
                               os.path.join(SAVE_DIR, f"imag_iter_{i}.pth"))
        
        # è®°å½•lossç”¨äºç»˜å›¾
        train_losses.append(avg_train_loss)
        test_losses.append(avg_test_loss)
        train_mse_losses.append(avg_train_mse)
        test_mse_losses.append(avg_test_mse)
        train_res_losses.append(avg_train_res)
        test_res_losses.append(avg_test_res)

        # ä¿å­˜åˆ°è®­ç»ƒæ•°æ®å­—å…¸
        training_data['epochs'].append(epoch)
        training_data['train_losses'].append(avg_train_loss)
        training_data['test_losses'].append(avg_test_loss)
        training_data['train_mse_losses'].append(avg_train_mse)
        training_data['test_mse_losses'].append(avg_test_mse)
        training_data['train_res_losses'].append(avg_train_res)
        training_data['test_res_losses'].append(avg_test_res)

        # ä¿å­˜hybrid lossæ•°æ®
        if LOSS_TYPE == "hybrid":
            hybrid_loss_data['epochs'].append(epoch)
            hybrid_loss_data['train_mse_losses'].append(avg_train_mse)
            hybrid_loss_data['train_phi_losses'].append(avg_train_phi)
            hybrid_loss_data['test_mse_losses'].append(avg_test_mse)
            hybrid_loss_data['test_phi_losses'].append(avg_test_phi)

        # å…¨å±€æ—©åœæ£€æŸ¥ï¼ˆé’ˆå¯¹æ‰€æœ‰é˜¶æ®µï¼‰
        if epoch >= EARLY_STOPPING_START_EPOCH:
            early_stopping_enabled = True
            if avg_test_loss < early_stopping_best_loss - EARLY_STOPPING_MIN_DELTA:
                # æœ‰æ˜¾è‘—æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
                early_stopping_best_loss = avg_test_loss
                early_stopping_counter = 0
            else:
                # æ— æ˜¾è‘—æ”¹å–„ï¼Œè®¡æ•°å™¨åŠ 1
                early_stopping_counter += 1

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ—©åœæ¡ä»¶
            if early_stopping_counter >= EARLY_STOPPING_PATIENCE:
                if is_main_process:
                    print(f"ğŸ›‘ å…¨å±€æ—©åœæ¿€æ´»ï¼")
                    print(f"   è¿ç»­{EARLY_STOPPING_PATIENCE}ä¸ªepochæ— æ˜¾è‘—æ”¹å–„")
                    print(f"   æœ€å°æ”¹å–„é˜ˆå€¼: {EARLY_STOPPING_MIN_DELTA:.0e}")
                    print(f"   å½“å‰loss: {avg_test_loss:.6e}")
                    print(f"   æœ€ä½³loss: {early_stopping_best_loss:.6e} (epoch {best_epoch})")
                break
        else:
            # é¢„çƒ­é˜¶æ®µï¼Œè·Ÿè¸ªæœ€ä½³lossä½†ä¸è§¦å‘æ—©åœ
            if avg_test_loss < early_stopping_best_loss:
                early_stopping_best_loss = avg_test_loss


        
        if epoch % EPOCH_PRINT == 0 and is_main_process:
            current_time = time.time()
            interval_time = current_time - last_print_time
            last_print_time = current_time

            # ä¸ºhybrid lossæ·»åŠ æƒé‡ä¿¡æ¯
            loss_info = f"Train Loss: {avg_train_loss:.6e} | Test Loss: {avg_test_loss:.6e}"

            print(f"Epoch {epoch:4d} | {loss_info} | "
                  f"Train RelErr: {avg_train_res:.6e} | Test RelErr: {avg_test_res:.6e} | "
                  f"Interval: {interval_time:.1f}s")
    
    # è®­ç»ƒç»“æŸï¼Œè¾“å‡ºæ€»è€—æ—¶
    if is_main_process:
        total_time = time.time() - total_start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = int(total_time % 60)
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {hours:02d}:{minutes:02d}:{seconds:02d} ({total_time:.1f}ç§’)")

        # è¾“å‡ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¢«ä¿å­˜ï¼ˆé€šè¿‡best_epochæˆ–best_saved_lossåˆ¤æ–­ï¼‰
        if best_epoch >= 0 and best_saved_loss < float('inf'):
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹:")
            print(f"   ğŸ¯ Epoch: {best_epoch}")
            print(f"   ğŸ“Š Test Loss: {best_saved_loss:.6e}")
            print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {SAVE_DIR}")
        else:
            print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹ï¼ˆå¯èƒ½è®­ç»ƒå¤±è´¥ï¼‰")
            print(f"   ğŸ“‚ æ¨¡å‹ä¿å­˜ç›®å½•: {SAVE_DIR}")

    # åªåœ¨ä¸»è¿›ç¨‹ä¸­ä¿å­˜ç»“æœå’Œç”Ÿæˆå›¾è¡¨
    if is_main_process:
        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_data_path = os.path.join(OUTPUT_DIR, "training_data.json")
        save_training_data(training_data, training_data_path)

        # ç”Ÿæˆè®­ç»ƒæ›²çº¿
        curve_pdf_path = os.path.join(OUTPUT_DIR, "training_curve.pdf")
        curve_svg_path = os.path.join(OUTPUT_DIR, "training_curve.svg")
        plot_training_curve(train_losses, test_losses, curve_pdf_path)

        # ç”ŸæˆMSEå’ŒRES lossæ›²çº¿
        mse_res_path = os.path.join(OUTPUT_DIR, "mse_res_loss.svg")
        plot_mse_res_loss(train_mse_losses, test_mse_losses, train_res_losses, test_res_losses, mse_res_path)

        # ä¿å­˜hybrid lossçš„MSEå’ŒPhi lossåˆ°txtæ–‡ä»¶ï¼ˆæ¯50è½®ä¿å­˜ä¸€æ¬¡ï¼‰
        if LOSS_TYPE == "hybrid":
            hybrid_loss_file = os.path.join(OUTPUT_DIR, "hybrid_loss_components.txt")
            with open(hybrid_loss_file, 'w') as f:
                f.write("Hybrid Loss Components (MSE + 1*Phi) - Every 50 epochs\n")
                f.write("="*60 + "\n")
                f.write("Epoch\tTrain_MSE\tTrain_Phi\tTest_MSE\tTest_Phi\n")
                # æ¯50è½®ä¿å­˜ä¸€æ¬¡
                for i, epoch in enumerate(hybrid_loss_data['epochs']):
                    if epoch % 50 == 0:  # æ¯50è½®ä¿å­˜ä¸€æ¬¡
                        f.write(f"{epoch}\t{hybrid_loss_data['train_mse_losses'][i]:.6e}\t")
                        f.write(f"{hybrid_loss_data['train_phi_losses'][i]:.6e}\t")
                        f.write(f"{hybrid_loss_data['test_mse_losses'][i]:.6e}\t")
                        f.write(f"{hybrid_loss_data['test_phi_losses'][i]:.6e}\n")

            print(f"âœ… Hybrid lossåˆ†é‡å·²ä¿å­˜åˆ°: {hybrid_loss_file}")

    # è®¡ç®—ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ
    if is_main_process:
        print("\næ­£åœ¨è®¡ç®—ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ...")
    raw_model = solver.module if isinstance(solver, (DataParallel, DDP)) else solver
    # æ ¹æ®æ¨¡å‹ç²¾åº¦å†³å®šçŸ©é˜µç²¾åº¦ - å…¼å®¹ä¸åŒå±‚ç±»å‹
    if hasattr(raw_model.model_real.networks[0].gcn1, 'conv'):
        model_precision = raw_model.model_real.networks[0].gcn1.conv.lin_fusion.weight.dtype
    elif hasattr(raw_model.model_real.networks[0].gcn1, 'spatial_conv'):
        model_precision = raw_model.model_real.networks[0].gcn1.spatial_conv.lin_fusion.weight.dtype
    elif hasattr(raw_model.model_real.networks[0].gcn1, 'linear'):
        model_precision = raw_model.model_real.networks[0].gcn1.linear.weight.dtype
    else:
        model_precision = torch.float32

    matrix_dtype = torch.complex128 if model_precision == torch.float64 else torch.complex64

    train_relative_errors = compute_relative_errors(solver, train_loader, data_mapping, device, matrix_dtype)
    test_relative_errors = compute_relative_errors(solver, test_loader, data_mapping, device, matrix_dtype)

    # ä¿å­˜ç›¸å¯¹è¯¯å·®æ•°æ®
    training_data['train_relative_errors'] = train_relative_errors
    training_data['test_relative_errors'] = test_relative_errors

    # è®¡ç®—MSE lossåˆ†å¸ƒï¼ˆä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„MSE losså€¼ï¼‰
    if is_main_process:
        print("\næ­£åœ¨ç”ŸæˆMSE lossåˆ†å¸ƒå›¾...")
    train_mse_samples = [loss for loss in train_mse_losses for _ in range(10)]  # é‡å¤å€¼ä»¥è·å¾—æ›´å¥½çš„åˆ†å¸ƒ
    test_mse_samples = [loss for loss in test_mse_losses for _ in range(10)]    # é‡å¤å€¼ä»¥è·å¾—æ›´å¥½çš„åˆ†å¸ƒ

    # ç”ŸæˆMSE lossåˆ†å¸ƒå›¾
    mse_dist_path = os.path.join(OUTPUT_DIR, "mse_loss_distribution.svg")
    plot_mse_loss_distribution(train_mse_samples, test_mse_samples, mse_dist_path)

def main():
    """ä¸»å‡½æ•°ï¼šå¯åŠ¨DDPè®­ç»ƒ"""
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPUï¼Œæ— æ³•è¿›è¡ŒDDPè®­ç»ƒ")
        return

    world_size = torch.cuda.device_count()
    if world_size == 0:
        print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•GPU")
        return

    print(f"ğŸš€ å¯åŠ¨DDPè®­ç»ƒï¼Œä½¿ç”¨ {world_size} ä¸ªGPU")

    # ä½¿ç”¨spawnæ–¹å¼å¯åŠ¨å¤šè¿›ç¨‹
    try:
        mp.spawn(main_worker, args=(world_size,), nprocs=world_size, join=True)
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ DDPè®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()